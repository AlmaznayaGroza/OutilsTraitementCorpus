"""Module principal de cr√©ation d'un corpus multilingue cyrillique

Ce module impl√©mente un syst√®me avanc√© de collecte et de curation de corpus
pour les langues utilisant l'alphabet cyrillique. Il d√©veloppe une approche
m√©ticuleuse, qui adapte automatiquement les strat√©gies de collecte
selon les sp√©cificit√©s et la disponibilit√© des ressources de chaque
langue trait√©e.

Innovation m√©thodologique principale:
    Le syst√®me impl√©mente une strat√©gie de collecte adaptative qui classe
    les langues cyrilliques en groupes selon leur richesse en ressources
    num√©riques, puis adapte les param√®tres de collecte, les profondeurs
    d'exploration, et les strat√©gies d'√©chantillonnage pour optimiser
    la qualit√© et la repr√©sentativit√© du corpus final pour chaque langue.

Architecture de collecte multi-strat√©gies:
    * collecte par cat√©gories th√©matiques (avec traductions adapt√©es)
    * exploration adaptive des sous-cat√©gories selon la profondeur optimale
    * √©chantillonnage al√©atoire pour garantir la diversit√© textuelle
    * validation qualit√© temps-r√©el avec filtrage des contenus inad√©quats
    * gestion intelligente des timeouts et reprises apr√®s interruption

Groupes linguistiques et strat√©gies adaptatives:
    - groupe A (langues bien dot√©es): exploration superficielle mais large
    - groupe B (langues interm√©diaires): √©quilibre exploration/profondeur
    - groupe C (langues minoritaires): exploration profonde et exhaustive
    - groupe D (langues tr√®s peu dot√©es): collecte maximale avec seuils abaiss√©s

Contr√¥le qualit√© et robustesse:
    Le syst√®me int√®gre des m√©canismes sophistiqu√©s de validation de contenu,
    de d√©duplication, de gestion d'erreurs et de reprise apr√®s interruption.
    Il veille au respect des bonnes pratiques d'utilisation d'APIs,
    avec gestion intelligente des d√©lais et limitation du d√©bit.

Sortie et m√©triques:
    Production de corpus √©quilibr√©s avec m√©tadonn√©es compl√®tes,
    statistiques d√©taill√©es par langue et m√©thode de collecte,
    et rapports de qualit√© permettant l'√©valuation
    de la repr√©sentativit√© linguistique obtenue.

Applications:
    Particuli√®rement adapt√© pour la cr√©ation de corpus d'entra√Ænement
    √©quilibr√©s pour mod√®les de TAL multilingues, √©tudes linguistiques
    comparatives, et recherches sur les langues √† ressources limit√©es.

Architecture:
    Utilise une approche modulaire avec s√©paration claire des responsabilit√©s:
        - configuration centralis√©e
        - collecte d'articles
        - traitement du texte
        - gestion des donn√©es
        - calcul de statistiques
        - gestion de cache

Note sur l'√©thique de collecte:
    Tous les contenus collect√©s respectent les conditions d'utilisation
    de Wikipedia et les bonnes pratiques de scraping √©thique avec limitation
    du d√©bit et respect des serveurs sources.
"""

import os
import time
import logging
from datetime import datetime
from typing import Optional, Tuple, List

# Import des modules refactoris√©s
from config import (
    TIME_LIMIT,
    LANGUAGES,
    ALL_CATEGORIES,
    MAX_DEPTHS_BY_GROUP,
    CATEGORY_TRANSLATIONS,
    get_adaptive_params,
    get_language_group,
    get_target_for_language,
)
from article_collector import ArticleCollector
from data_manager import save_articles_to_csv
from stat_manager import (
    CollectionStats,
    calculate_token_targets,
    print_collection_plan,
    save_global_stats,
)


# =============================================================
# CONSTANTES SUPPLEMENTAIRES DE CONFIGURATION POUR LA COLLECTE
# =============================================================

# Configuration des dossiers et fichiers
DEFAULT_PATHS = {
    "base_dir": "data",
    "articles_dir": "data/raw",
    "logs_dir": "logs",
    "metrics_base": "results/metrics/corpus_analysis/collection",
    "languages_stats_dir": "results/metrics/corpus_analysis/collection/languages",
    "global_stats_dir": "results/metrics/corpus_analysis/collection/global",
}

# Param√®tres de collecte et performance
COLLECTION_PARAMS = {
    "max_languages_limit": None,  # None = toutes les langues, sinon nombre entier
    "batch_sizes": {
        "A": {"initial": 15, "min": 5},  # langues bien dot√©es
        "B": {"initial": 25, "min": 10},  # langues interm√©diaires
        "C": {"initial": 30, "min": 10},  # langues minoritaires
        "D": {"initial": 30, "min": 10},  # langues rares
    },
    "max_attempts": 15,  # tentatives max par cat√©gorie
    "timeout_multiplier": 1.0,  # multiplicateur pour TIME_LIMIT
}

# Configuration du logging
LOGGING_CONFIG = {
    "filename_pattern": "cyrillique_collecte.log",
    "level": logging.INFO,
    "format": "%(asctime)s - %(levelname)s - %(message)s",
    "console_level": logging.WARNING,
}

# Messages d'information standardis√©s
PROGRESS_MESSAGES = {
    "start_collection": "=== D√âMARRAGE DE LA COLLECTE SUR {} LANGUES ===",
    "language_progress": "Langue {}/{}: {}",
    "collection_complete": "=== COLLECTE TERMIN√âE ===",
    "total_summary": "Total: {} articles collect√©s pour {} langues",
    "phase_1": "1. Collecte d'articles des cat√©gories principales (objectif: {} tokens)",
    "phase_2": "2. Collecte d'articles des sous-cat√©gories (objectif: {} tokens)",
    "phase_3": "3. Collecte d'articles al√©atoires (objectif: {} tokens)",
}


# =====================================
# FONCTIONS UTILITAIRES ET VALIDATION
# =====================================


def validate_collection_parameters(
    max_languages: Optional[int] = None, timeout_multiplier: float = 1.0
) -> None:
    """Valide les param√®tres de collecte avant le d√©marrage

    Cette fonction v√©rifie la coh√©rence des param√®tres de collecte pour
    √©viter les erreurs d'ex√©cution et optimiser les performances selon
    les ressources disponibles et les objectifs de collecte.

    Args:
        max_languages (int, optional): limite le nombre de langues √† traiter
        timeout_multiplier (float): multiplicateur pour ajuster les timeouts

    Raises:
        ValueError: si les param√®tres sont incoh√©rents ou invalides

    Note:
        Cette validation pr√©ventive permet d'identifier rapidement les
        probl√®mes de configuration et d'√©viter les √©checs tardifs durant
        le processus de collecte, qui dure des heures.
    """
    if max_languages is not None:
        if not isinstance(max_languages, int) or max_languages < 1:
            raise ValueError("max_languages doit √™tre un entier positif")
        if max_languages > len(LANGUAGES):
            raise ValueError(
                f"max_languages ({max_languages}) d√©passe "
                f"le nombre de langues disponibles ({len(LANGUAGES)})"
            )

    if (
        not isinstance(timeout_multiplier, (int, float))
        or timeout_multiplier <= 0
    ):
        raise ValueError("timeout_multiplier doit √™tre un nombre positif")

    if timeout_multiplier > 2.0:
        logging.warning(
            f"timeout_multiplier √©lev√© ({timeout_multiplier}x) "
            f"- collecte tr√®s longue attendue"
        )


def setup_collection_environment() -> None:
    """Configure l'environnement de collecte avec tous les dossiers n√©cessaires

    Cette fonction initialise l'infrastructure n√©cessaire pour la collecte:
    dossier de donn√©es, syst√®me de logging, et validation de l'environnement
    d'ex√©cution. Elle garantit que tous les pr√©requis sont satisfaits avant
    le d√©marrage de la collecte.

    Raises:
        PermissionError: si impossible de cr√©er les dossiers requis
        RuntimeError: si l'environnement ne peut pas √™tre initialis√©
    """
    try:
        # Cr√©ation des r√©pertoires de donn√©es
        for path_key, path_value in DEFAULT_PATHS.items():
            os.makedirs(path_value, exist_ok=True)

        # Configuration du syst√®me de logging
        log_path = os.path.join(
            DEFAULT_PATHS["logs_dir"], LOGGING_CONFIG["filename_pattern"]
        )
        logging.basicConfig(
            filename=log_path,
            level=LOGGING_CONFIG["level"],
            format=LOGGING_CONFIG["format"],
        )

        # Ajout du logging console pour les messages importants
        console_handler = logging.StreamHandler()
        console_handler.setLevel(LOGGING_CONFIG["console_level"])
        logging.getLogger("").addHandler(console_handler)

        logging.info("Environnement de collecte initialis√© avec succ√®s")

    except PermissionError as e:
        raise PermissionError(f"Impossible de cr√©er les dossiers n√©cessaires: {e}")
    except Exception as e:
        logging.error(
            f"Erreur critique dans la fonction principale: {str(e)}",
            exc_info=True
        )
        raise RuntimeError(f"√âchec de la collecte: {e}") from e


def get_adaptive_batch_size(
    language_code: str, remaining_tokens: int, attempt_number: int
) -> int:
    """Calcule une taille de batch adaptative selon la langue et le contexte

    Cette fonction impl√©mente une logique sophistiqu√©e qui adapte la taille
    des batches de collecte selon plusieurs facteurs: le groupe linguistique,
    le nombre de tokens restants √† collecter et le nombre de tentatives
    d√©j√† effectu√©es. Cette adaptation optimise l'efficacit√© de la collecte.

    Args:
        language_code (str): code de la langue en cours de traitement
        remaining_tokens (int): nb de tokens restants √† collecter
        attempt_number (int): num√©ro de la tentative actuelle

    Returns:
        int: taille de batch optimale pour cette situation

    Note m√©thodologique:
        Cette adaptation permet d'optimiser la collecte en commen√ßant par
        des batches plus grands puis en les r√©duisant si n√©cessaire, tout
        en respectant les caract√©ristiques sp√©cifiques de chaque groupe
        linguistique identifi√© dans la recherche.
    """
    group = get_language_group(language_code)
    batch_config = COLLECTION_PARAMS["batch_sizes"].get(
        group, COLLECTION_PARAMS["batch_sizes"]["C"]
    )

    # Calcul de base selon le groupe linguistique
    if attempt_number == 1:
        base_size = batch_config["initial"]
    else:
        # R√©duction progressive avec les tentatives
        base_size = max(
            batch_config["min"],
            batch_config["initial"] - (attempt_number - 1) * 2
        )

    # Adaptation selon les tokens restants
    if remaining_tokens < 1000:
        adjusted_size = max(batch_config["min"], base_size // 2)
    elif remaining_tokens > 5000:
        adjusted_size = min(base_size + 5, batch_config["initial"] + 10)
    else:
        adjusted_size = base_size

    return adjusted_size


def calculate_effective_timeout(
        base_timeout: int,
        multiplier: float = 1.0
) -> int:
    """Calcule le timeout effectif avec validation des limites raisonnables

    Args:
        base_timeout (int): timeout de base en secondes
        multiplier (float): multiplicateur √† appliquer

    Returns:
        int: timeout effectif en secondes,
            born√© dans des limites raisonnables
    """
    effective_timeout = int(base_timeout * multiplier)

    # Borner dans des limites raisonnables (1h min, 6h max)
    min_timeout = 3600  # 1h
    max_timeout = 21600  # 6h

    return max(min_timeout, min(effective_timeout, max_timeout))


# ==========================================
# FONCTION PRINCIPALE DE COLLECTE OPTIMISEE
# ==========================================


def collect_articles(language_code: str, categories: List[str]) -> Tuple:
    """
    Fonction de collecte avec adaptation par groupe de langue
    et validation robuste

    Cette fonction impl√©mente le c≈ìur de la logique de collecte adaptative
    d√©velopp√©e dans ce projet. Elle orchestre les 3 phases de collecte
    (cat√©gories principales, sous-cat√©gories, articles al√©atoires) avec des
    param√®tres optimis√©s selon le groupe linguistique de la langue trait√©e.

    Args:
        language_code (str): code ISO de la langue √† traiter
        categories (List[str]): liste des cat√©gories th√©matiques √† explorer

    Returns:
        Tuple: (articles, stats_cat√©gories, total_tokens, temps_ex√©cution,
                main_ordered_tokens, main_random_tokens, limited_articles,
                pourcentage_limit√©)

    Raises:
        ValueError: si les param√®tres d'entr√©e sont invalides
        RuntimeError: si la collecte √©choue de mani√®re critique

    Note sur la m√©thodologie:
        Cette fonction repr√©sente l'innovation principale du projet avec son
        syst√®me adaptatif qui optimise automatiquement les strat√©gies de
        collecte selon les sp√©cificit√©s de chaque langue, permettant de
        cr√©er des corpus plus √©quilibr√©s et repr√©sentatifs.
    """
    logging.info(f"D√©marrage de la collecte pour la langue: {language_code}")

    try:
        # R√©cup√©ration et validation des param√®tres adaptatifs
        group = get_language_group(language_code)
        token_target = get_target_for_language(language_code)
        params = get_adaptive_params(language_code)

        if not categories:
            raise ValueError(f"Aucune cat√©gorie fournie pour {language_code}")

        # D√©termination des cat√©gories disponibles pour cette langue
        available_categories = []
        for category in categories:
            if language_code in CATEGORY_TRANSLATIONS.get(category, {}):
                available_categories.append(category)

        print(
            f"\nCat√©gories disponibles ({len(available_categories)}/{len(categories)}): "
            f"{', '.join(available_categories)}"
        )

        if not available_categories:
            logging.warning(
                f"Attention: aucune cat√©gorie disponible pour {language_code}"
            )
            return [], {}, 0, 0, 0, 0, 0, 0

        # Initialisation des stats avec validation
        stats = CollectionStats(language_code, token_target, categories)

        # Calcul et d√©finition des objectifs de tokens
        targets = calculate_token_targets(
            token_target,
            params,
            available_categories
        )

        (main_target,
         sub_target,
         random_target,
         tokens_per_main,
         tokens_per_sub
         ) = targets

        stats.set_token_targets(
            main_target,
            sub_target,
            random_target,
            tokens_per_main,
            tokens_per_sub
        )

        # Affichage du plan de collecte pour tra√ßabilit√©
        print_collection_plan(
            language_code,
            group,
            token_target,
            params,
            targets
        )

        # Variables de suivi de la collecte
        start_time = time.time()
        articles = []
        collected_article_ids = set()

        # Initialisation du collecteur d'articles
        collector = ArticleCollector(
            language_code=language_code,
            categories=categories,
            params=params,
            already_collected_ids=collected_article_ids,
        )

        # =============================================
        # ETAPE 1: Collecte des CAT√âGORIES PRINCIPALES
        # =============================================
        print(
            f"\n{PROGRESS_MESSAGES['phase_1'].format(
                stats.main_category_token_target
            )}"
        )

        for category in available_categories:
            # V√©rification du timeout global
            effective_timeout = calculate_effective_timeout(
                TIME_LIMIT, COLLECTION_PARAMS["timeout_multiplier"]
            )

            if time.time() - start_time > effective_timeout:
                print(
                    f"Limite de temps atteinte pour {language_code}. "
                    f"Passage √† la langue suivante."
                )
                break

            translated_category = CATEGORY_TRANSLATIONS[category][language_code]
            print(f"\n  Cat√©gorie: {category} ({translated_category})")

            # Collecte adaptative avec gestion d'erreurs
            category_target = stats.tokens_per_main_category.get(
                category, tokens_per_main
            )
            category_tokens = 0
            attempts = 0

            while (
                category_tokens < category_target
                and attempts < COLLECTION_PARAMS["max_attempts"]
            ):
                attempts += 1
                remaining_tokens = category_target - category_tokens

                # Calcul adaptatif de la taille de batch
                batch_size = get_adaptive_batch_size(
                    language_code, remaining_tokens, attempts
                )

                print(
                    f"  Tentative {attempts}: recherche de {batch_size} articles "
                    f"(objectif: {remaining_tokens} tokens manquants)"
                )

                try:
                    (
                        category_articles,
                        tokens_collected,
                        ordered_tokens,
                        random_tokens,
                    ) = collector.collect_by_category(
                        category_name=translated_category,
                        category_target=category_target,
                        num_articles=batch_size,
                        fixed_ratio=params["fixed_selection_ratio"],
                        sleep_time=(1, 2.5),
                    )

                    if not category_articles:
                        print(
                            f"  Aucun article disponible pour {category}"
                            f" ({translated_category})"
                        )
                        break

                    # Enregistrement et mise √† jour des statistiques
                    stats.set_available_articles(
                        category, len(category_articles)
                    )
                    articles.extend(category_articles)

                    # Mise √† jour des stats agr√©g√©es ordered/random
                    stats.main_ordered_tokens += ordered_tokens
                    stats.main_random_tokens += random_tokens

                    for article in category_articles:
                        stats.update_main_category_stats(
                            category, article, article.get("type")
                        )

                    category_tokens += tokens_collected

                    print(
                        f"  Progression: {category_tokens}/{category_target}"
                        f" tokens collect√©s"
                    )

                except Exception as e:
                    logging.error(
                        f"Erreur lors de la collecte pour {category}: "
                        f"{str(e)}",
                        exc_info=True,
                    )
                    continue

        # Affichage du r√©sum√© de l'√©tape 1
        stats.print_progress_summary(
            "les cat√©gories principales",
            stats.main_category_tokens,
            stats.main_category_token_target,
        )

        # ======================================
        # ETAPE 2: Collecte des SOUS-CAT√âGORIES
        # ======================================
        if time.time() - start_time <= effective_timeout:
            print(
                f"\n{PROGRESS_MESSAGES['phase_2'].format(
                    stats.subcategory_token_target
                )}"
            )

            subcategories_cache = {}

            for category in available_categories:
                if time.time() - start_time > effective_timeout:
                    print(
                        "Limite de temps atteinte pendant la collecte des sous-cat√©gories."
                    )
                    break

                translated_category = CATEGORY_TRANSLATIONS[category][language_code]
                print(
                    f"\n  Sous-cat√©gories de: {category} "
                    f"({translated_category})"
                )

                category_target = stats.tokens_per_subcategory.get(
                    category, tokens_per_sub
                )
                category_tokens = 0
                attempts = 0
                cached_subcats = subcategories_cache.get(category, None)

                while (
                    category_tokens < category_target
                    and attempts < COLLECTION_PARAMS["max_attempts"]
                ):
                    attempts += 1
                    remaining_tokens = category_target - category_tokens
                    batch_size = get_adaptive_batch_size(
                        language_code, remaining_tokens, attempts
                    )

                    print(
                        f"  Tentative {attempts}: recherche de "
                        f"{batch_size} articles"
                    )

                    try:
                        max_depth = MAX_DEPTHS_BY_GROUP[group]

                        subcategory_articles, updated_cache, sub_tokens = (
                            collector.collect_from_subcategories(
                                category_name=translated_category,
                                num_articles=batch_size,
                                max_depth=max_depth,
                                sleep_time=(0.5, 1.5),
                                cached_subcategories=cached_subcats,
                                attempt_number=attempts,
                                token_target=remaining_tokens,
                            )
                        )

                        if updated_cache:
                            subcategories_cache[category] = updated_cache
                            cached_subcats = updated_cache

                        if not subcategory_articles:
                            continue

                        articles.extend(subcategory_articles)

                        for article in subcategory_articles:
                            stats.update_subcategory_stats(category, article)

                        category_tokens += sub_tokens
                        print(
                            f"  Progression: {category_tokens}/{category_target} "
                            f"tokens collect√©s"
                        )

                    except Exception as e:
                        logging.error(
                            f"Erreur lors de la collecte des sous-cat√©gories "
                            f"pour {category}: {str(e)}",
                            exc_info=True,
                        )
                        break

            stats.print_progress_summary(
                "les sous-cat√©gories",
                stats.subcategory_tokens,
                stats.subcategory_token_target,
            )

        # ========================================
        # ETAPE 3: Collecte d'ARTICLES AL√âATOIRES
        # ========================================
        if time.time() - start_time <= effective_timeout:
            print(
                f"\n{PROGRESS_MESSAGES['phase_3'].format(
                    stats.random_token_target
                )}"
            )

            try:
                random_tokens = 0

                while random_tokens < stats.random_token_target:
                    if time.time() - start_time > effective_timeout:
                        print(
                            "Limite de temps atteinte pendant la collecte des articles al√©atoires."
                        )
                        break

                    remaining_tokens = stats.random_token_target - random_tokens
                    batch_size = max(1, min(15, remaining_tokens // 200))

                    random_articles, rand_tokens = collector.collect_random(
                        num_articles=batch_size, sleep_time=(0.5, 2)
                    )

                    if not random_articles:
                        print(
                            f"  Plus d'articles al√©atoires disponibles "
                            f"pour {language_code}"
                        )
                        break

                    articles.extend(random_articles)

                    for article in random_articles:
                        stats.update_random_stats(article)

                    random_tokens += rand_tokens
                    print(
                        f"  Progression: "
                        f"{random_tokens}/{stats.random_token_target}"
                        f" tokens collect√©s"
                    )

                    if random_tokens >= stats.random_token_target:
                        break

                stats.print_progress_summary(
                    "les articles al√©atoires",
                    stats.random_tokens,
                    stats.random_token_target,
                )

            except Exception as e:
                logging.error(
                    f"Erreur lors de la collecte des articles al√©atoires: "
                    f"{str(e)}",
                    exc_info=True,
                )

        # ============================================
        # FINALISATION ET G√âN√âRATION DES STATISTIQUES
        # ============================================

        total_tokens = stats.total_tokens
        execution_time = time.time() - start_time

        logging.info(
            f"Collecte termin√©e pour {language_code}: "
            f"{len(articles)} articles, {total_tokens} tokens"
        )

        # Sauvegarde des statistiques d√©taill√©es
        stats.save_to_file(
            execution_time,
            params["max_token_length"],
            output_dir=DEFAULT_PATHS["languages_stats_dir"],
        )

        logging.info(f"Statistiques finales pour {language_code} sauvegard√©es")

        return (
            articles,
            stats.categories_stats,
            stats.total_tokens,
            execution_time,
            stats.main_ordered_tokens,
            stats.main_random_tokens,
            stats.limited_articles,
            (
                stats.limited_articles / stats.articles_count * 100
                if stats.articles_count > 0
                else 0
            ),
        )

    except Exception as e:
        logging.error(
            f"Erreur critique lors de la collecte pour {language_code}: "
            f"{str(e)}",
            exc_info=True,
        )
        raise RuntimeError(
            f"√âchec de la collecte pour {language_code}: "
            f"{e}"
        ) from e


# =====================
# FONCTION PRINCIPALE
# =====================

def main(
        max_languages: Optional[int] = None,
        timeout_multiplier: float = 1.0
) -> List:
    """
    Fonction principale pour ex√©cuter la collecte adaptative avec robustesse

    Cette fonction orchestre l'ensemble du processus de collecte pour toutes
    les langues configur√©es, avec gestion robuste des erreurs, reprise apr√®s
    interruption, et g√©n√©ration de rapports complets sur la qualit√© et les
    performances de la collecte r√©alis√©e.

    Args:
        max_languages (int, optional): limite le nb de langues √† traiter
            (si None, traite toutes les langues configur√©es)
        timeout_multiplier (float): multiplicateur pour ajuster les d√©lais
            (valeur par d√©faut: 1.0 (timeouts standards))

    Returns:
        List: liste de tous les articles collect√©s avec m√©tadonn√©es compl√®tes

    Raises:
        ValueError: si les param√®tres d'entr√©e sont invalides
        RuntimeError: si l'initialisation de l'environnement √©choue

    Exemple:
        Collecte standard sur toutes les langues:
        >>> articles = main()

        Collecte limit√©e avec timeouts prolong√©s:
        >>> articles = main(max_languages=5, timeout_multiplier=1.5)

    Note sur la robustesse:
        La fonction continue le traitement m√™me si certaines langues
        √©chouent, permettant de maximiser la collecte r√©ussie tout en
        loggant les probl√®mes pour analyse ult√©rieure.
    """
    logging.info("D√©marrage de la fonction principale de collecte")

    try:
        # Validation des param√®tres et initialisation
        validate_collection_parameters(max_languages, timeout_multiplier)
        setup_collection_environment()

        # Configuration de la collecte
        languages_to_process = (
            LANGUAGES[:max_languages] if max_languages
            else LANGUAGES
        )
        max_languages_count = len(languages_to_process)

        print(PROGRESS_MESSAGES["start_collection"].format(max_languages_count))

        # Variables de suivi global
        all_articles = []
        successfully_processed = set()
        failed_languages = []

        # Boucle principale de collecte par langue
        for i, language in enumerate(languages_to_process, 1):
            if language in successfully_processed:
                print(
                    f"\nLa langue {language} a d√©j√† √©t√© trait√©e, "
                    f"passage √† la suivante."
                )
                continue

            print("\n" + "=" * 50)
            print(
                PROGRESS_MESSAGES["language_progress"].format(
                    i, max_languages_count, language
                )
            )
            print("=" * 50)

            try:
                # Collecte pour cette langue avec gestion d'erreurs
                collection_result = collect_articles(language, ALL_CATEGORIES)

                (
                    articles,
                    cat_stats,
                    total_tokens,
                    exec_time,
                    main_ordered_tokens,
                    main_random_tokens,
                    limited_articles,
                    limited_percentage,
                ) = collection_result

                if articles:
                    # Ajout √† la collection globale
                    all_articles.extend(articles)

                    # Sauvegarde des articles dans un fichier CSV
                    save_articles_to_csv(
                        language, articles, DEFAULT_PATHS["articles_dir"]
                    )

                    print(
                        f"‚úÖ {len(articles)} articles collect√©s pour {language}"
                        f", {total_tokens} tokens"
                    )
                    successfully_processed.add(language)
                else:
                    print(f"‚ö†Ô∏è Aucun article collect√© pour {language}")
                    failed_languages.append((language, "Aucun article collect√©"))

            except Exception as e:
                error_msg = f"Erreur lors de la collecte pour {language}: {str(e)}"
                logging.error(error_msg, exc_info=True)
                failed_languages.append((language, str(e)))
                print(f"‚ùå {error_msg}")
                continue

        # =====================================================
        # G√âN√âRATION DU RAPPORT FINAL ET STATISTIQUES GLOBALES
        # =====================================================
        print("\n" + "=" * 60)
        print(PROGRESS_MESSAGES["collection_complete"])
        print("=" * 60)
        print(
            PROGRESS_MESSAGES["total_summary"].format(
                len(all_articles), len(successfully_processed)
            )
        )

        # Statistiques de succ√®s/√©chec
        success_rate = len(successfully_processed) / max_languages_count * 100
        print(
            f"Taux de r√©ussite: {success_rate:.1f}% "
            f"({len(successfully_processed)}/{max_languages_count} langues)"
        )

        if failed_languages:
            print(f"\n‚ö†Ô∏è Langues avec probl√®mes ({len(failed_languages)}):")
            for lang, error in failed_languages:
                print(f"  ‚Ä¢ {lang}: {error}")

        # Sauvegarde des statistiques globales
        global_stats_path = f"{DEFAULT_PATHS['global_stats_dir']}/global_stats.csv"
        save_global_stats(
            successfully_processed,
            all_articles,
            global_stats_path
        )

        print(f"\nüìä Statistiques globales sauvegard√©es : {global_stats_path}")
        print(f"üìÅ Articles sauvegard√©s dans : {DEFAULT_PATHS['articles_dir']}")

        return all_articles

    except KeyboardInterrupt:
        print("\n‚ùå Collecte interrompue par l'utilisateur")
        logging.info("Collecte interrompue par l'utilisateur")
        raise
    except Exception as e:
        logging.error(
            f"Erreur critique dans la fonction principale: {str(e)}",
            exc_info=True
        )
        raise RuntimeError(f"√âchec de la collecte: {e}") from e


# =========================================================
# POINT D'ENTR√âE PRINCIPAL AVEC GESTION D'ERREURS COMPL√àTE
# =========================================================

if __name__ == "__main__":
    """Point d'entr√©e principal avec gestion d'erreurs robuste
    et interface utilisateur

    Ex√©cute la collecte de corpus avec gestion compl√®te des erreurs,
    logging appropri√©, et interface utilisateur informative
    pour le monitoring en temps r√©el du processus de collecte.

    Variables d'environnement prises en charge:
        MAX_LANGUAGES: limite le nombre de langues √† traiter
        TIMEOUT_MULTIPLIER: ajuste les d√©lais d'attente
            (par d√©faut: 1.0)

    Codes de sortie:
        0: succ√®s total
        1: interruption utilisateur
        2: erreur de param√®tres ou d'environnement
        3: erreur de collecte partielle
        4: √©chec critique
    """
    try:
        print("üöÄ Lancement de la cr√©ation de corpus cyrillique adaptatif...")
        print(f"üìÖ D√©marrage: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # Lecture des param√®tres d'environnement optionnels
        max_langs = None
        if "MAX_LANGUAGES" in os.environ:
            try:
                max_langs = int(os.environ["MAX_LANGUAGES"])
                print(
                    f"üî¢ Limitation √† {max_langs} langues "
                    f"(variable d'environnement)"
                )
            except ValueError:
                print("‚ö†Ô∏è Variable MAX_LANGUAGES invalide, ignor√©e")

        timeout_mult = 1.0
        if "TIMEOUT_MULTIPLIER" in os.environ:
            try:
                timeout_mult = float(os.environ["TIMEOUT_MULTIPLIER"])
                print(f"‚è±Ô∏è Multiplicateur de timeout : {timeout_mult}x")
            except ValueError:
                print(
                    "‚ö†Ô∏è Variable TIMEOUT_MULTIPLIER invalide, valeur par d√©faut utilis√©e"
                )

        # Lancement de la collecte principale
        articles = main(
            max_languages=max_langs,
            timeout_multiplier=timeout_mult
        )

        # Rapport de succ√®s final
        end_time = datetime.now()
        print("\n‚úÖ Collecte termin√©e avec succ√®s !")
        print(f"üìä {len(articles):,} articles collect√©s au total")
        print(f"‚è±Ô∏è Termin√© : {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üìÅ Donn√©es sauvegard√©es dans : {DEFAULT_PATHS['articles_dir']}")
        print(f"üìà M√©triques disponibles dans : {DEFAULT_PATHS['metrics_base']}")

        # Message de conclusion
        print("\nüéØ Mission accomplie! Corpus multilingue cyrillique cr√©√© avec succ√®s.")

    except KeyboardInterrupt:
        print("\n‚ùå Processus interrompu par l'utilisateur (Ctrl+C)")
        print("üíæ Les donn√©es partielles ont √©t√© sauvegard√©es")
        exit(1)

    except (ValueError, RuntimeError) as e:
        print(f"\n‚ùå Erreur de configuration ou d'ex√©cution: {e}")
        print("üí° V√©rifiez les param√®tres et les permissions de fichiers")
        logging.error(f"Erreur de configuration: {e}")
        exit(2)

    except Exception as e:
        print(f"\n‚ùå Erreur critique inattendue : {e}")
        print("üí° Consultez les logs pour plus de d√©tails")
        print(
            f"üìù Fichier de log: {DEFAULT_PATHS['logs_dir']}"
            f"/{LOGGING_CONFIG['filename_pattern']}"
        )
        logging.error(f"Erreur critique: {e}", exc_info=True)
        exit(4)

    # Succ√®s complet
    exit(0)
