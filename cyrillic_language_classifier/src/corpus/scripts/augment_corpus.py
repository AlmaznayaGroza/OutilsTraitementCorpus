"""Module d'augmentation de donn√©es pour corpus multilingues cyrilliques

Ce module fournit des outils pour l'augmentation de donn√©es textuelles
dans les langues utilisant l'alphabet cyrillique. Il permet de g√©n√©rer du
contenu synth√©tique de qualit√© pour enrichir des corpus de langues
sous-repr√©sent√©es et am√©liorer l'√©quilibrage des datasets multilingues.

Le module contient la classe CyrillicDataAugmenter, qui impl√©mente
3 strat√©gies d'augmentation linguistiquement motiv√©es :
    * g√©n√©ration de texte synth√©tique bas√©e sur des mod√®les n-grammes adaptatifs
    * cr√©ation d'articles multilingues par m√©lange de langues linguistiquement proches
    * perturbation de texte par substitution et suppression de caract√®res motiv√©es

Architecture adaptive:
    Le syst√®me d'augmentation adapte automatiquement ses param√®tres selon le groupe
    linguistique de chaque langue (familles slaves, turciques, finno-ougriennes, etc.)
    pour optimiser la qualit√© des donn√©es g√©n√©r√©es tout en respectant les
    sp√©cificit√©s orthographiques et morphologiques de chaque langue.

Innovation m√©thodologique:
    Les strat√©gies impl√©ment√©es s'appuient sur des principes de linguistique
    computationnelle pour g√©n√©rer du contenu qui pr√©serve les caract√©ristiques
    stylistiques et structurelles des langues sources, permettant un enrichissement
    de corpus qui maintient la validit√© linguistique des donn√©es augment√©es.

Applications :
    Ce module est particuli√®rement adapt√© aux projets multilingues, o√π
    l'√©quilibrage des corpus est crucial pour l'entra√Ænement de mod√®les √©quitables
    et performants sur des langues √† ressources limit√©es.
"""

import pandas as pd
import numpy as np
import random
import os
import re
from collections import Counter
import nltk
from nltk.util import ngrams
import glob

# T√©l√©chargement des ressources NLTK n√©cessaires
try:
    nltk.download("punkt", quiet=True)
except Exception as e:
    print("Impossible de t√©l√©charger les ressources NLTK: {e}. Continuer sans...")


# ===========================================================
# CONSTANTES DE CONFIGURATION POUR L'AUGMENTATION DE CORPUS
# ===========================================================

# Langues des groupes C et D (langues tr√®s minoritaires)
# N√©cessitent un traitement sp√©cial avec g√©n√©ration de n-grammes
MINORITY_LANGUAGES = ["ab", "kbd", "koi", "kv", "mhr"]

# Paires de langues linguistiquement proches pour g√©n√©ration multilingue
# Organis√©es par famille linguistique pour maximiser la coh√©rence
LANGUAGE_PAIRS = [
    # Langues slaves de l'Est
    ("ru", "uk"),  # russe - ukrainien
    ("ru", "be"),  # russe - b√©larussien
    ("uk", "be"),  # ukrainien - b√©larussien
    ("uk", "rue"),  # ukrainien - rusyn
    ("be", "rue"),  # b√©larussien - rusyn
    # Langues slaves du Sud
    ("bg", "mk"),  # bulgare - mac√©donien
    ("sr", "bg"),  # serbe - bulgare
    ("mk", "sr"),  # mac√©donien - serbe
    # Langues turciques
    ("kk", "ky"),  # kazakh - kirghize
    ("tt", "ba"),  # tatar - bachkir
    # Langues finno-ougriennes (priorit√© pour les groupes C et D)
    ("koi", "kv"),  # komi-permyak - komi (prioritaire)
    ("udm", "koi"),  # oudmourte - komi-permyak (prioritaire)
    ("udm", "kv"),  # oudmourte - komi (prioritaire)
    ("myv", "mhr"),  # erzya - mari (prioritaire)
    # Langues caucasiennes
    ("ab", "kbd"),  # abkhaze - kabarde (minoritaires)
    ("ab", "ce"),  # abkhaze - tch√©tch√®ne (minoritaire avec majoritaire)
    ("kbd", "ce"),  # kabardien - tch√©tch√®ne (minoritaire avec majoritaire)
]

# Paires prioritaires (groupes C et D) qui g√©n√®rent plus d'articles
PRIORITY_PAIRS = [
    ("koi", "kv"),
    ("udm", "koi"),
    ("udm", "kv"),
    ("myv", "mhr"),
    ("ab", "kbd"),
    ("ab", "ce"),
    ("kbd", "ce"),
]

# Substitutions de caract√®res pour la perturbation de texte
# Bas√©es sur les variations orthographiques entre langues proches
CHAR_SUBSTITUTIONS = {
    # Langues slaves de l'Est
    "ru": {"–∏": "—ñ", "–µ": "—î", "—ç": "—î", "—ã": "–∏", "–≥": "“ë"},
    "uk": {"—ñ": "e", "–∏": "—ã", "—ó": "–π", "—î": "e", "–µ": "—ç", "—å": "'", "—â": "—à—á"},
    "be": {
        "—ñ": "–∏",
        "—ã": "–∏",
        "—û": "–≤",
        "—ç": "–µ",
        "—è": "–µ",
        "'": "—ä",
        "–∞": "–æ",
        "—à—á": "—â",
        "–≥": "“ë",
        "—ë": "–µ",
        "—Ü": "—Ç",
        "–¥–∑": "–¥",
    },
    "rue": {
        "—ã": "–∏",
        "–∏": "—ñ",
        "—ó": "–π",
        "–µ": "—î",
        "—ç": "–µ",
        "—ë": "–æ",
        "–≥": "“ë",
        "—å": "'",
        "—£": "–µ",
    },
    # Langues slaves du Sud
    "bg": {
        "—ä": "–æ",
        "—è": "—ò–∞",
        "—å–æ": "—ò–æ",
        "—é": "—ò—É",
        "—â": "—à—Ç",
        "—å": "—ò",
        "–∂": "—ü",
        "–Ω–µ": "—ö–µ",
        "–ª–µ": "—ô–µ",
    },
    "mk": {
        "—ò": "–π",
        "—ú": "—â",
        "—ì": "–∂–¥",
        "—ò–∞": "–µ",
        "—ü": "–∂",
        "–æ": "—ä",
        "–∞": "—ä",
        "—ö": "–Ω",
        "—ô–µ": "–ª–µ",
    },
    # Langues du groupe C - caract√®res sp√©ciaux
    "ab": {"“ß": "–ø", "“µ": "—Ü", "”∑": "–≥"},
    "kbd": {"”è": "—ñ", "—ç": "–µ", "—â": "—à—á"},
    "koi": {"”ß": "–æ", "—ñ": "–∏"},
    "kv": {"”ß": "–æ", "—ñ": "–∏"},
    # Langue du groupe D
    "mhr": {"”±": "—É", "”ß": "–æ", "”π": "—ã"},
    # Substitutions par d√©faut pour toutes les autres langues
    "default": {"–µ": "—ç", "–∏": "–π", "–æ": "–∞"},
}

# Multiplicateur d'intensit√© de perturbation par langue
# Les langues minoritaires ont une intensit√© plus √©lev√©e
# pour compenser leur sous-repr√©sentation
PERTURBATION_INTENSITY = {
    "ab": 3,
    "kbd": 3,
    "koi": 3,
    "kv": 3,
    "mhr": 3,  # groupes C et D
    "default": 1,  # toutes les autres langues
}

# Param√®tres de configuration
SEGMENT_LENGTH_DEFAULT = 50
MAX_SEGMENTS_PER_ARTICLE = 8
MIN_MIXED_TEXT_LENGTH = 50
NGRAM_TEXT_LENGTH = 100


# ======================
# FONCTIONS UTILITAIRES
# ======================


def create_text_segments(article, segment_length=SEGMENT_LENGTH_DEFAULT):
    """D√©coupe un article en segments de longueur donn√©e

    Cette fonction prend un article et le divise en morceaux plus petits
    pour faciliter la recombinaison lors de la g√©n√©ration synth√©tique.

    Args:
        article (str): texte de l'article √† d√©couper
        segment_length (int): longueur des segments en mots

    Returns:
        list: liste des segments de texte
    """
    if not isinstance(article, str) or not article.strip():
        return []

    words = article.split()
    segments = []
    actual_segment_length = min(segment_length, len(words) // 2)

    for i in range(0, len(words), actual_segment_length):
        if i + actual_segment_length <= len(words):
            segments.append(" ".join(words[i: i + actual_segment_length]))

    return segments


def split_text_into_sentences(text):
    """Divise un texte en phrases pour le m√©lange multilingue

    Utilise une approche simple bas√©e sur la ponctuation pour identifier
    les limites de phrases dans les textes cyrilliques.

    Args:
        text (str): texte √† diviser

    Returns:
        list: liste des phrases non vides
    """
    if not isinstance(text, str) or not text.strip():
        return []

    # D√©couper en phrases (approximation bas√©e sur la ponctuation)
    sentences = re.split(r"[.!?]+", text)

    # Filtrer les phrases vides et nettoyer
    return [s.strip() for s in sentences if s.strip()]


def get_language_target_ratio(lang):
    """D√©termine le ratio de longueur cible selon la langue

    Les langues avec plus de donn√©es peuvent avoir des articles synth√©tiques
    plus courts, tandis que les langues rares n√©cessitent
    des articles plus longs.

    Args:
        lang (str): code de la langue

    Returns:
        float: ratio de longueur (entre 0 et 1)
    """
    # Groupe A: langues avec beaucoup de donn√©es
    if lang in ["sr", "ba", "ru", "mk"]:
        return 0.85  # 85% de la longueur originale
    # Groupe B: langues interm√©diaires
    elif lang in ["bxr", "tyv", "rue", "ab", "be"]:
        return 0.90  # 90% de la longueur originale
    else:
        return 0.95  # 95% de la longueur originale (groupes C et D)


class CyrillicDataAugmenter:
    """Cette classe impl√©mente plusieurs strat√©gies d'augmentation de donn√©es
    pour enrichir des corpus de langues cyrilliques sous-repr√©sent√©es.

    Attributes:
        input_dir (str): dossier contenant les fichiers CSV d'entr√©e
        output_dir (str): dossier de sortie pour les donn√©es augment√©es
        char_ngram_models (dict): mod√®les de n-grammes de caract√®res par langue
        word_ngram_models (dict): mod√®les de n-grammes de mots par langue
        articles_by_language (dict): articles originaux organis√©s par langue

    Methods:
        load_data(): charge les donn√©es depuis les fichiers CSV
        build_language_models(): construit des mod√®les statistiques par langue
        generate_synthetic_dataset(): g√©n√®re des articles synth√©tiques
        generate_cross_language_articles(): cr√©e des articles multilingues
        data_perturbation(): applique des perturbations aux textes existants
        augment_data(): ex√©cute le processus complet d'augmentation

    Examples:
        Utilisation basique :
            >>> augmenter = CyrillicDataAugmenter()
            >>> corpus_augmente = augmenter.augment_data()

        Contr√¥le fin des param√®tres :
            >>> augmenter = CyrillicDataAugmenter(
            ...     input_dir="mon_corpus",
            ...     output_dir="corpus_augmente"
            ... )
            >>> synthetics = augmenter.generate_synthetic_dataset(
            ...     count_per_language=25
            ...     )
    """

    def __init__(
        self, input_dir="data/processed/merged",
        output_dir="data/processed/augmented"
    ):
        """Initialise l'augmenteur de donn√©es

        Args:
            input_dir (str, optional): dossier contenant les fichiers CSV
                d'articles initiaux (par d√©faut: "data/processed/merged")
            output_dir (str, optional): dossier de sortie pour sauvegarder
                les donn√©es augment√©es (par d√©faut: "data/processed/augmented")

        Raises:
            OSError: si impossible de cr√©er le r√©pertoire de sortie
        """
        if not os.path.exists(input_dir):
            raise FileNotFoundError(
                f"Le r√©pertoire d'entr√©e '{input_dir}' n'existe pas"
            )

        self.input_dir = input_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # Dictionnaires pour stocker les mod√®les de langue par langue
        self.char_ngram_models = {}  # mod√®les de n-grammes de caract√®res
        self.word_ngram_models = {}  # mod√®les de n-grammes de mots
        self.articles_by_language = {}  # articles originaux par langue

    def load_data(self):
        """Charge les donn√©es depuis les fichiers CSV

        Lit tous les fichiers CSV du dossier d'entr√©e et les organise
        par langue. Chaque fichier doit √™tre nomm√© selon le format:
        "{code_langue}_articles.csv".

        Raises:
            FileNotFoundError: si le r√©pertoire d'entr√©e n'existe pas
            ValueError: si aucun fichier CSV valide n'est trouv√©

        Note:
            les fichiers sans colonne 'text' sont ignor√©s avec un avertissement
        """
        print("Chargement des donn√©es originales...")

        all_files = glob.glob(f"{self.input_dir}/*_articles.csv")

        if not all_files:
            raise FileNotFoundError(f"Aucun fichier CSV trouv√© dans {self.input_dir}")

        for file in all_files:
            # Extraction du code de langue depuis le nom de fichier
            lang_code = os.path.basename(file).split("_")[0]

            try:
                df = pd.read_csv(file)

                # V√©rifier que les colonnes n√©cessaires existent
                if "text" not in df.columns:
                    print(f"Colonne 'text' manquante dans {file}, fichier ignor√©")
                    continue

                # Filtrer les lignes avec du texte non vide
                df = df[df["text"].notna() & (df["text"] != "")]

                if df.empty:
                    print(f"Aucun texte valide dans {file}, fichier ignor√©")
                    continue

                # Stocker les articles
                self.articles_by_language[lang_code] = df
                print(f"  Charg√© {len(df)} articles pour {lang_code}")

            except Exception as e:
                print(f"Erreur lors du chargement de {file}: {e}")
                continue

        if not self.articles_by_language:
            raise ValueError("Aucun fichier CSV valide n'a pu √™tre charg√©")

        print(f"Donn√©es charg√©es pour {len(self.articles_by_language)} langues")

    def build_language_models(self):
        """Construit des mod√®les statistiques pour chaque langue

        Cr√©e des mod√®les de n-grammes de caract√®res et de mots pour chaque
        langue charg√©e. Ces mod√®les servent de base pour la g√©n√©ration
        de texte synth√©tique.

        Les mod√®les incluent :
            - n-grammes de caract√®res (bi, tri, quadri-grammes)
            - n-grammes de mots (uni, bi, tri-grammes)

        Raises:
            ValueError: si aucune donn√©e n'a √©t√© charg√©e avant l'appel

        Note:
            En cas d'erreur sur une langue sp√©cifique, un mod√®le vide
            est cr√©√© et le traitement continue pour les autres langues
        """
        print("Construction des mod√®les de langue...")

        for lang, articles_df in self.articles_by_language.items():
            print(f"  Mod√©lisation de la langue {lang}...")

            try:
                # Concat√©ner tous les textes
                all_text = " ".join(articles_df["text"].fillna(""))

                if not all_text.strip():
                    print(f"    Aucun texte disponible pour {lang}")
                    continue

                # Mod√®le de n-grammes de caract√®res
                # (pour capturer les sch√©mas orthographiques)
                char_ngram_dict = {}
                for n in range(2, 5):  # bigrammes, trigrammes et quadrigrammes
                    char_ngram_dict[n] = Counter()
                    for i in range(len(all_text) - n + 1):
                        ngram = all_text[i: i + n]
                        char_ngram_dict[n][ngram] += 1

                self.char_ngram_models[lang] = char_ngram_dict

                # Mod√®le de n-grammes de mots (pour la structure syntaxique)
                try:
                    # Tokeniser par mots (approximation)
                    words = re.findall(r"\b\w+\b", all_text.lower())

                    word_ngram_dict = {}
                    for n in range(1, 4):  # unigrammes, bigrammes et trigrammes
                        word_ngram_dict[n] = Counter(ngrams(words, n))

                    self.word_ngram_models[lang] = word_ngram_dict
                    print(f"    Mod√®le construit avec {len(words)} mots")

                except Exception as e:
                    print(
                        f"    Erreur lors de la construction du mod√®le de mots pour {lang}: {e}"
                    )
                    # Cr√©er un mod√®le vide en cas d'erreur
                    self.word_ngram_models[lang] = {
                        n: Counter() for n in range(1, 4)
                    }

            except Exception as e:
                print(f"    Erreur lors de la mod√©lisation pour {lang}: {e}")
                # Cr√©er des mod√®les vides en cas d'erreur
                self.char_ngram_models[lang] = {
                    n: Counter() for n in range(2, 5)
                }
                self.word_ngram_models[lang] = {
                    n: Counter() for n in range(1, 4)
                }

        print("Tous les mod√®les de langue ont √©t√© construits!")

    def generate_synthetic_text(self, lang, length=250):
        """G√©n√®re un texte synth√©tique bas√© sur le mod√®le de langue

        Cr√©e un nouveau texte en combinant des segments d'articles existants
        et en appliquant optionnellement des mod√®les n-grammes pour les
        langues sous-repr√©sent√©es.

        Args:
            lang (str): code de la langue pour laquelle g√©n√©rer le texte
            length (int, optional): longueur cible en mots (par d√©faut: 250)

        Returns:
            str: texte synth√©tique g√©n√©r√©, ou cha√Æne vide en cas d'√©chec

        Note:
            Pour les langues des groupes C et D (langues tr√®s minoritaires),
            utilise √©galement un mod√®le de trigrammes de caract√®res pour
            augmenter la diversit√© du texte g√©n√©r√©
        """
        # V√©rifier que les mod√®les et articles existent
        if (
            lang not in self.char_ngram_models
            or lang not in self.word_ngram_models
            or lang not in self.articles_by_language
        ):
            print(f"Pas de mod√®le disponible pour la langue {lang}")
            return ""

        # Approche 1: utiliser des segments de textes existants
        original_articles = self.articles_by_language[lang]["text"].tolist()

        if not original_articles:
            return ""

        # S√©lectionner quelques articles al√©atoires
        selected_articles = random.sample(
            original_articles, min(3, len(original_articles))
        )

        # D√©couper les articles en segments plus courts
        segments = []
        for article in selected_articles:
            article_segments = create_text_segments(article)
            segments.extend(article_segments)

        # Si pas assez de segments, utiliser l'article complet
        if len(segments) < 3:
            segments = selected_articles

        # Approche 2: pour les langues minoritaires,
        # utiliser aussi le mod√®le n-grammes
        if lang in MINORITY_LANGUAGES:
            try:
                # Utiliser le mod√®le de trigrammes
                # pour g√©n√©rer du texte √† partir de z√©ro
                char_model = {}
                all_text = " ".join(original_articles)

                # Construire un mod√®le de trigrammes de caract√®res
                for i in range(len(all_text) - 3):
                    trigram = all_text[i:i+3]
                    next_char = all_text[i + 3]
                    if trigram not in char_model:
                        char_model[trigram] = []
                    char_model[trigram].append(next_char)

                # G√©n√©rer du texte avec ce mod√®le
                if char_model:
                    try:
                        # S√©lectionner un trigramme al√©atoire pour commencer
                        current = random.choice(list(char_model.keys()))
                        generated_text = current

                        # G√©n√©rer une centaine de caract√®res
                        for _ in range(NGRAM_TEXT_LENGTH):
                            if current in char_model and char_model[current]:
                                next_char = random.choice(char_model[current])
                                generated_text += next_char
                                current = current[1:] + next_char
                            else:
                                # Si le trigramme n'existe pas, prendre un autre au hasard
                                if char_model:
                                    current = random.choice(
                                        list(char_model.keys())
                                    )

                        # Ajouter ce texte g√©n√©r√© comme segment suppl√©mentaire
                        segments.append(generated_text)
                    except (IndexError, KeyError) as e:
                        # En cas d'erreur, ignorer la g√©n√©ration par n-grammes
                        print(f"    Erreur n-gramme pour {lang}: {e}")
                        pass
            except Exception as e:
                print(f"    Erreur lors de la g√©n√©ration n-gramme pour {lang}: {e}")

        # M√©langer et combiner des segments
        random.shuffle(segments)

        # Prendre suffisamment de segments pour atteindre la longueur cible
        synthetic_text = " ".join(
            segments[: min(MAX_SEGMENTS_PER_ARTICLE, len(segments))]
        )

        # Ajuster pour obtenir une longueur approximative
        words = synthetic_text.split()
        if len(words) > length:
            synthetic_text = " ".join(words[:length])

        # R√©p√©ter pour atteindre au moins 80% de la longueur cible
        attempts = 0
        max_attempts = 3  # √©viter les boucles infinies
        while (
            len(synthetic_text.split()) < 0.8 * length
            and len(segments) > 5
            and attempts < max_attempts
        ):
            random.shuffle(segments)
            additional_text = " ".join(
                segments[:3]
            )  # prendre 3 segments suppl√©mentaires
            synthetic_text += " " + additional_text

            # Rev√©rifier qu'on ne d√©passe pas la longueur cible
            words = synthetic_text.split()
            if len(words) > length:
                synthetic_text = " ".join(words[:length])
                break

            attempts += 1

        return synthetic_text

    def generate_synthetic_dataset(self, count_per_language=20):
        """G√©n√®re un ensemble de donn√©es synth√©tiques pour toutes les langues

        Cr√©e des articles synth√©tiques pour chaque langue charg√©e en utilisant
        des mod√®les de langue et des techniques de recombinaison de segments.

        Args:
            count_per_language (int, optional): nb d'articles √† g√©n√©rer
                par langue (par d√©faut: 20)

        Returns:
            pandas.DataFrame: DataFrame contenant
                tous les articles synth√©tiques avec les colonnes
                'language', 'title', 'text', 'token_count',
                'category', 'source'

        Note:
            La longueur cible des articles est ajust√©e selon la langue:
                - langues du groupe A: 85% de la longueur moyenne originale
                - langues du groupe B: 90%
                - langues des groupes C et D: 95%
        """
        print(f"G√©n√©ration de {count_per_language} articles synth√©tiques par langue...")

        all_synthetic_articles = []

        for lang in self.articles_by_language.keys():
            print(f"  G√©n√©ration pour {lang}...")

            try:
                # Calculer la longueur moyenne des articles originaux
                original_lengths = self.articles_by_language[lang][
                    "token_count"
                ].tolist()
                avg_length = (
                    int(np.mean(original_lengths)) if original_lengths else 200
                )

                # Utiliser notre fonction utilitaire pour le ratio de longueur
                target_ratio = get_language_target_ratio(lang)

                # G√©n√©rer des articles synth√©tiques
                articles_created = 0
                for i in range(count_per_language):
                    # Varier un peu la longueur
                    target_length = int(
                        avg_length * target_ratio * random.uniform(0.8, 1.2)
                    )

                    # G√©n√©rer le texte
                    synthetic_text = self.generate_synthetic_text(
                        lang, length=target_length
                    )

                    if synthetic_text:
                        # Cr√©er un article synth√©tique
                        synthetic_article = {
                            "language": lang,
                            "title": f"Synthetic_{lang}_{i+1}",
                            "text": synthetic_text,
                            "token_count": len(synthetic_text.split()),
                            "category": "Synthetic",
                            "source": "data_augmentation",
                        }

                        all_synthetic_articles.append(synthetic_article)
                        articles_created += 1

                print(f"    {articles_created} articles g√©n√©r√©s")

            except Exception as e:
                print(f"Erreur lors de la g√©n√©ration pour {lang}: {e}")
                continue

        # Cr√©er un DataFrame avec tous les articles synth√©tiques
        synthetic_df = pd.DataFrame(all_synthetic_articles)

        # Sauvegarder les donn√©es synth√©tiques
        try:
            output_file = f"{self.output_dir}/synthetic_articles.csv"
            synthetic_df.to_csv(output_file, index=False)
            print(f"Dataset synth√©tique cr√©√© avec {len(synthetic_df)} articles")
        except Exception as e:
            print(f"Erreur lors de la sauvegarde: {e}")

        return synthetic_df

    def generate_cross_language_articles(self, pairs=None, count_per_pair=5):
        """G√©n√®re des articles synth√©tiques en m√©langeant 2 langues proches

        Cr√©e des articles multilingues en combinant des phrases de langues
        linguistiquement proches pour simuler des ph√©nom√®nes de code-switching
        ou de multilinguisme.

        Args:
            pairs (list of tuples, optional): paires de langues √† m√©langer -
                Si None, utilise des paires pr√©d√©finies bas√©es sur la proximit√©
                linguistique (par d√©faut: None)
            count_per_pair (int, optional): nb d'articles √† g√©n√©rer par
                paire de langues - les paires prioritaires (groupes C et D)
                g√©n√®rent le double (par d√©faut: 5)

        Returns:
            pandas.DataFrame: DataFrame contenant les articles multilingues
                avec colonne suppl√©mentaire 'mixing_strategy'

        Note:
            2 strat√©gies de m√©lange sont utilis√©es:
                - Alternance: phrases altern√©es entre les 2 langues
                - Blocs: blocs cons√©cutifs de phrases par langue
        """
        # Si aucune paire n'est fournie
        if pairs is None:
            pairs = LANGUAGE_PAIRS

        print("G√©n√©ration d'articles synth√©tiques par m√©lange de langues...")
        mixed_articles = []

        for lang1, lang2 in pairs:
            # V√©rifier si les 2 langues sont disponibles
            if (
                lang1 not in self.articles_by_language
                or lang2 not in self.articles_by_language
            ):
                print(f"  Paire {lang1}-{lang2} ignor√©e: une des langues manque")
                continue

            # D√©terminer combien d'articles g√©n√©rer pour cette paire
            if (
                (lang1, lang2) in PRIORITY_PAIRS or
                (lang2, lang1) in PRIORITY_PAIRS
            ):
                pair_count = (
                    count_per_pair * 2
                )  # double d'articles pour les paires prioritaires
            else:
                pair_count = count_per_pair

            print(
                f"  M√©lange des langues {lang1} et {lang2} (objectif: {pair_count} articles)..."
            )

            try:
                # R√©cup√©rer des articles pour les 2 langues
                articles1 = self.articles_by_language[lang1]["text"].tolist()
                articles2 = self.articles_by_language[lang2]["text"].tolist()

                if not articles1 or not articles2:
                    continue

                # G√©n√©rer des articles m√©lang√©s
                articles_created = 0
                max_attempts = pair_count * 2  # permettre plus de tentatives

                for i in range(max_attempts):
                    if articles_created >= pair_count:
                        break

                    try:
                        # S√©lectionner des articles al√©atoires
                        article1 = random.choice(articles1)
                        article2 = random.choice(articles2)

                        # D√©couper en phrases
                        sentences1 = split_text_into_sentences(article1)
                        sentences2 = split_text_into_sentences(article2)

                        if not sentences1 or not sentences2:
                            continue

                        # M√©langer des phrases des 2 langues (2 strat√©gies)
                        mixed_sentences = []

                        # Strat√©gie 1: alternance simple (articles pairs)
                        if i % 2 == 0:
                            for j in range(
                                min(10, max(len(sentences1), len(sentences2)))
                            ):
                                if j % 2 == 0 and j // 2 < len(sentences1):
                                    mixed_sentences.append(sentences1[j // 2])
                                elif j // 2 < len(sentences2):
                                    mixed_sentences.append(sentences2[j // 2])

                        # Strat√©gie 2: blocs de phrases (articles impairs)
                        else:
                            # 1er bloc: langue 1
                            start_idx = random.randint(
                                0, max(0, len(sentences1) - 3)
                            )
                            end_idx = min(start_idx + 3, len(sentences1))
                            mixed_sentences.extend(
                                sentences1[start_idx:end_idx]
                            )

                            # 2√®me bloc: langue 2
                            start_idx = random.randint(
                                0, max(0, len(sentences2) - 3)
                            )
                            end_idx = min(start_idx + 3, len(sentences2))
                            mixed_sentences.extend(
                                sentences2[start_idx:end_idx]
                            )

                            # 3√®me bloc: langue 1
                            if len(sentences1) > end_idx + 3:
                                mixed_sentences.extend(
                                    sentences1[end_idx:end_idx+2]
                                )

                        # Cr√©er le texte m√©lang√©
                        mixed_text = ". ".join(mixed_sentences)

                        # S'assurer que le texte est suffisamment long
                        if len(mixed_text.split()) < MIN_MIXED_TEXT_LENGTH:
                            continue

                        # Ajouter l'article m√©lang√©
                        mixed_article = {
                            "language": f"{lang1}_{lang2}_mix",
                            "title": f"Mixed_{lang1}_{lang2}_{articles_created+1}",
                            "text": mixed_text,
                            "token_count": len(mixed_text.split()),
                            "category": "Mixed_Language",
                            "source": "cross_language_augmentation",
                            "mixing_strategy": (
                                "alternating" if i % 2 == 0 else "blocks"
                            ),
                        }

                        mixed_articles.append(mixed_article)
                        articles_created += 1

                    except Exception as e:
                        print(
                            f"    Erreur lors de la cr√©ation d'un article m√©lang√©: {e}"
                        )
                        continue

                print(f"    {articles_created} articles m√©lang√©s cr√©√©s")

            except Exception as e:
                print(f"Erreur lors du traitement de la paire {lang1}-{lang2}: {e}")
                continue

        # Cr√©er un DataFrame avec les articles m√©lang√©s
        mixed_df = pd.DataFrame(mixed_articles)

        # Sauvegarder les donn√©es
        if not mixed_df.empty:
            try:
                output_file = f"{self.output_dir}/mixed_language_articles.csv"
                mixed_df.to_csv(output_file, index=False)
                print(f"  {len(mixed_df)} articles de langues m√©lang√©es cr√©√©s au total")
            except Exception as e:
                print(f"Erreur lors de la sauvegarde: {e}")
        else:
            print("  Aucun article de langues m√©lang√©es n'a pu √™tre cr√©√©")

        return mixed_df

    def data_perturbation(self, character_swap_prob=0.01, deletion_prob=0.01):
        """
        Cr√©e des variations des articles existants par perturbation l√©g√®re du texte.

        Simule des erreurs typographiques et des variations dialectales en
        appliquant des substitutions et suppressions de caract√®res contr√¥l√©es.

        Args:
            character_swap_prob (float, optional): probabilit√© de remplacer
                un caract√®re par un √©quivalent linguistique (par d√©faut: 0.01)
            deletion_prob (float, optional): probabilit√© de supprimer un
                caract√®re (par d√©faut: 0.01)

        Returns:
            pandas.DataFrame: DataFrame contenant les articles perturb√©s
                avec colonne suppl√©mentaire 'original_id'

        Note:
            L'intensit√© de perturbation est tripl√©e pour les langues des
            groupes C et D (ab, kbd, koi, kv, mhr) pour compenser leur
            sous-repr√©sentation dans le corpus original.

            Les substitutions de caract√®res sont linguistiquement motiv√©es
            (ex: variations orthographiques entre langues proches).
        """
        print("G√©n√©ration d'articles par perturbation de texte...")

        all_perturbed_articles = []

        for lang, articles_df in self.articles_by_language.items():
            print(f"  Perturbation pour {lang}...")

            try:
                # D√©finir des substitutions sp√©cifiques √† cette langue
                lang_subs = CHAR_SUBSTITUTIONS.get(
                    lang, CHAR_SUBSTITUTIONS["default"]
                )

                # Intensit√© de perturbation pour la langue
                intensity = PERTURBATION_INTENSITY.get(
                    lang, PERTURBATION_INTENSITY["default"]
                )

                # Ajuster les probabilit√©s selon l'intensit√©
                adjusted_swap_prob = character_swap_prob * intensity
                adjusted_deletion_prob = deletion_prob * intensity

                # Nb d'articles √† perturber (plus pour les langues prioritaires)
                if lang in MINORITY_LANGUAGES:
                    sample_size = min(20, len(articles_df))
                else:
                    sample_size = min(10, len(articles_df))

                # Pour chaque article, cr√©er une version perturb√©e
                for _, article in articles_df.sample(sample_size).iterrows():
                    try:
                        text = article["text"]

                        if not isinstance(text, str) or not text:
                            continue

                        perturbed_text = ""
                        for char in text:
                            # Possibilit√© de supprimer un caract√®re
                            if random.random() < adjusted_deletion_prob:
                                continue

                            # Possibilit√© de remplacer un caract√®re
                            if random.random() < adjusted_swap_prob:
                                if char.lower() in lang_subs:
                                    replacement = lang_subs[char.lower()]
                                    # Pr√©server la casse
                                    if char.isupper():
                                        replacement = replacement.upper()
                                    char = replacement

                            perturbed_text += char

                        # S'assurer que le texte perturb√© n'est pas vide
                        if not perturbed_text.strip():
                            continue

                        # Cr√©er l'article perturb√©
                        perturbed_article = {
                            "language": lang,
                            "title": f"Perturbed_{article.get('title', 'Unknown')}",
                            "text": perturbed_text,
                            "token_count": len(perturbed_text.split()),
                            "category": "Perturbed",
                            "source": "data_perturbation",
                            "original_id": article.get("id", ""),
                        }

                        all_perturbed_articles.append(perturbed_article)

                    except Exception as e:
                        print(f"    Erreur lors de la perturbation d'un article: {e}")
                        continue

                articles_for_lang = [
                    a for a in all_perturbed_articles if a["language"] == lang
                ]
                print(
                    f"    {len(articles_for_lang)} articles perturb√©s cr√©√©s pour {lang}"
                )

            except Exception as e:
                print(f"Erreur lors de la perturbation pour {lang}: {e}")
                continue

        # Cr√©er un DataFrame avec tous les articles perturb√©s
        perturbed_df = pd.DataFrame(all_perturbed_articles)

        # Sauvegarder les donn√©es
        if not perturbed_df.empty:
            try:
                output_file = f"{self.output_dir}/perturbed_articles.csv"
                perturbed_df.to_csv(output_file, index=False)
                print(
                    f"Dataset d'articles perturb√©s cr√©√© avec {len(perturbed_df)} articles"
                )
            except Exception as e:
                print(f"Erreur lors de la sauvegarde: {e}")
        else:
            print("Aucun article perturb√© n'a pu √™tre cr√©√©")

        return perturbed_df

    def combine_augmented_datasets(self, synthetic_df, mixed_df, perturbed_df):
        """
        Combine tous les datasets augment√©s en un seul.

        Fusionne les DataFrames des diff√©rentes strat√©gies d'augmentation
        (synth√©tique, multilingue, perturb√©) en un corpus unifi√©.

        Args:
            synthetic_df (pandas.DataFrame): articles synth√©tiques
            mixed_df (pandas.DataFrame): articles multilingues
            perturbed_df (pandas.DataFrame): articles perturb√©s

        Returns:
            pandas.DataFrame: corpus augment√© complet combinant toutes
                les strat√©gies d'augmentation (ou DataFrame vide si aucune
                donn√©e d'augmentation n'est disponible)

        Note:
            Le DataFrame r√©sultant est automatiquement sauvegard√© sous
            le nom "all_augmented_articles.csv" dans le r√©pertoire de sortie
        """
        print("Combinaison de tous les datasets augment√©s...")

        # Liste pour stocker tous les DataFrames
        all_dfs = []

        # Ajouter chaque DataFrame non vide
        if synthetic_df is not None and not synthetic_df.empty:
            all_dfs.append(synthetic_df)

        if mixed_df is not None and not mixed_df.empty:
            all_dfs.append(mixed_df)

        if perturbed_df is not None and not perturbed_df.empty:
            all_dfs.append(perturbed_df)

        # Combiner tous les DataFrames
        if all_dfs:
            try:
                combined_df = pd.concat(all_dfs, ignore_index=True)
                output_file = f"{self.output_dir}/all_augmented_articles.csv"
                combined_df.to_csv(output_file, index=False)
                print(f"Dataset augment√© complet cr√©√© avec {len(combined_df)} articles")
                return combined_df
            except Exception as e:
                print(f"Erreur lors de la combinaison: {e}")
                return pd.DataFrame()
        else:
            print("Aucun article augment√© n'a √©t√© cr√©√©")
            return pd.DataFrame()

    def augment_data(
            self,
            synthetic_count=20,
            mixed_pairs=None,
            mixed_count=5
    ):
        """
        Ex√©cute le processus complet d'augmentation des donn√©es.

        Lance s√©quentiellement toutes les √©tapes d'augmentation:
        chargement des donn√©es, construction des mod√®les,
        g√©n√©ration synth√©tique, cr√©ation d'articles multilingues,
        perturbation, et combinaison finale.

        Args:
            synthetic_count (int, optional): nb d'articles synth√©tiques
                √† g√©n√©rer par langue (par d√©faut: 20)
            mixed_pairs (list of tuples, optional): paires de langues pour
                la g√©n√©ration multilingue - si None, utilise des paires
                pr√©d√©finies (par d√©faut: None)
            mixed_count (int, optional): nb d'articles multilingues
                √† g√©n√©rer par paire de langues (par d√©faut: 5)

        Returns:
            pandas.DataFrame: corpus augment√© complet combinant toutes
                les strat√©gies d'augmentation

        Raises:
            FileNotFoundError: si le dossier d'entr√©e n'existe pas
            ValueError: si aucune donn√©e valide n'est trouv√©e

        Exemples:
            Augmentation avec param√®tres par d√©faut :
                >>> augmenter = CyrillicDataAugmenter()
                >>> corpus = augmenter.augment_data()

            Augmentation personnalis√©e :
                >>> corpus = augmenter.augment_data(
                ...     synthetic_count=30,
                ...     mixed_count=10
                ... )
        """
        try:
            print("=== D√©but du processus d'augmentation des donn√©es ===")

            # 1. Charger les donn√©es
            print("\n1. Chargement des donn√©es...")
            self.load_data()

            # 2. Construire les mod√®les de langue
            print("\n2. Construction des mod√®les de langue...")
            self.build_language_models()

            # 3. G√©n√©rer des articles synth√©tiques
            print("\n3. G√©n√©ration d'articles synth√©tiques...")
            synthetic_df = self.generate_synthetic_dataset(
                count_per_language=synthetic_count
            )

            # 4. G√©n√©rer des articles de langues m√©lang√©es
            print("\n4. G√©n√©ration d'articles multilingues...")
            mixed_df = self.generate_cross_language_articles(
                pairs=mixed_pairs, count_per_pair=mixed_count
            )

            # 5. G√©n√©rer des articles perturb√©s
            print("\n5. G√©n√©ration d'articles perturb√©s...")
            perturbed_df = self.data_perturbation()

            # 6. Combiner tous les datasets
            print("\n6. Combinaison des datasets...")
            combined_df = self.combine_augmented_datasets(
                synthetic_df, mixed_df, perturbed_df
            )

            # 7. Afficher un r√©sum√©
            self._print_summary(
                synthetic_df,
                mixed_df,
                perturbed_df,
                combined_df
            )

            print("\n=== Processus d'augmentation termin√© avec succ√®s ===")
            return combined_df

        except Exception as e:
            print(f"\nErreur lors du processus d'augmentation: {e}")
            raise

    def _print_summary(self, synthetic_df, mixed_df, perturbed_df, combined_df):
        """Affiche un r√©sum√© d√©taill√© de l'augmentation"""
        print("\n" + "=" * 50)
        print("R√âSUM√â DE L'AUGMENTATION DES DONN√âES")
        print("=" * 50)

        synthetic_count = len(synthetic_df) if synthetic_df is not None else 0
        mixed_count = len(mixed_df) if mixed_df is not None else 0
        perturbed_count = len(perturbed_df) if perturbed_df is not None else 0
        total_count = len(combined_df) if combined_df is not None else 0

        print(f"Articles synth√©tiques g√©n√©r√©s     : {synthetic_count:,}")
        print(f"Articles multilingues cr√©√©s       : {mixed_count:,}")
        print(f"Articles perturb√©s cr√©√©s          : {perturbed_count:,}")
        print("-" * 50)
        print(f"TOTAL des articles augment√©s      : {total_count:,}")

        if combined_df is not None and not combined_df.empty:
            # Statistiques par langue si disponibles
            if "language" in combined_df.columns:
                lang_counts = combined_df["language"].value_counts()
                print(f"\nNombre de langues augment√©es      : {len(lang_counts)}")
                print("\nTop 5 des langues les plus augment√©es :")
                for lang, count in lang_counts.head().items():
                    print(f"  {lang}: {count:,} articles")

            # Statistiques par m√©thode si disponibles
            if "source" in combined_df.columns:
                source_counts = combined_df["source"].value_counts()
                print("\nDistribution par m√©thode d'augmentation :")
                for source, count in source_counts.items():
                    percentage = (count / total_count) * 100
                    print(f"  {source}: {count:,} articles ({percentage:.1f}%)")


# Point d'entr√©e principal du script
if __name__ == "__main__":
    """Point d'entr√©e principal du script avec gestion d'erreurs

    Ex√©cute l'augmentation de donn√©es avec les param√®tres par d√©faut
    si le module est lanc√© directement depuis la ligne de commande.

    Usage:
        python augment_corpus.py
    """
    try:
        print("Lancement de l'augmentation de corpus cyrillique...")

        augmenter = CyrillicDataAugmenter(
            input_dir="data/processed/merged",
            output_dir="data/processed/augmented"
        )

        corpus_augmente = augmenter.augment_data()

        print("\n‚úÖ Augmentation termin√©e avec succ√®s !")
        print(f"{len(corpus_augmente)} articles augment√©s g√©n√©r√©s")
        print("üìÅ R√©sultats sauvegard√©s dans data/processed/augmented/")

    except KeyboardInterrupt:
        print("\n‚ùå Processus interrompu par l'utilisateur")
        exit(1)
    except FileNotFoundError as e:
        print(f"\n‚ùå Fichier/dossier non trouv√©: {e}")
        print(
            "üí° V√©rifiez que le r√©pertoire d'entr√©e existe et contient des fichiers CSV"
        )
        exit(1)
    except ValueError as e:
        print(f"\n‚ùå Erreur de donn√©es: {e}")
        print("üí° V√©rifiez le format de vos fichiers CSV")
        exit(1)
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        print("üí° Consultez la documentation")
        exit(1)
