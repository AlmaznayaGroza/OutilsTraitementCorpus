"""Module de division stratifi√©e de corpus multilingues pour l'apprentissage automatique

Ce module fournit des outils sp√©cialis√©s pour diviser intelligemment un corpus
multilingue en ensembles d'entra√Ænement, validation et test. Il prend en compte
certaines sp√©cificit√©s des langues cyrilliques et les d√©s√©quilibres naturels des corpus.

Le module impl√©mente une strat√©gie de division adaptative qui:
    * pr√©serve la repr√©sentativit√© linguistique dans chaque ensemble
    * g√®re les langues avec peu d'exemples par des strat√©gies sp√©ciales
    * maintient l'√©quilibre entre donn√©es originales et augment√©es
    * calcule des m√©triques de qualit√© de la division (entropie, statistiques)

Architecture de division:
    La division respecte une hi√©rarchie langue > source > √©chantillonnage al√©atoire
    pour garantir que chaque ensemble contient une repr√©sentation √©quitable de
    toutes les langues et sources de donn√©es disponibles.
    Les langues minoritaires b√©n√©ficient de strat√©gies de division adapt√©es
    pour maximiser leur utilit√© tout en maintenant la validit√© statistique
    des ensembles cr√©√©s.

Gestion des cas particuliers :
    Le module traite les situations o√π certaines langues ont tr√®s peu d'exemples,
    en adaptant les ratios de division et en appliquant des seuils minimaux
    pour maintenir la coh√©rence m√©thodologique.

Sortie standardis√©e :
    Les ensembles g√©n√©r√©s respectent les conventions d'organisation des donn√©es
    pour l'apprentissage automatique, avec des r√©pertoires s√©par√©s et des
    statistiques d√©taill√©es pour faciliter l'√©valuation de la qualit√© de la division.
"""


import os
import pandas as pd
import numpy as np


# ========================================================
# CONSTANTES DE CONFIGURATION POUR LA DIVISION DU CORPUS
# ========================================================

# Ratios par d√©faut pour la division train/validation/test
DEFAULT_TRAIN_RATIO = 0.8
DEFAULT_VAL_RATIO = 0.1
DEFAULT_TEST_RATIO = 0.1

# Seuils pour la gestion des petits datasets
MIN_EXAMPLES_FOR_NORMAL_SPLIT = 10  # assez d'exs pour une division significative
MIN_EXAMPLES_FOR_ANY_SPLIT = 3  # minimum pour diviser en plusieurs ensembles
MIN_EXAMPLES_FOR_DUAL_SPLIT = 2  # minimum pour diviser en 2 ensembles

# Param√®tres de validation
RATIO_SUM_TOLERANCE = 1e-10  # tol√©rance pour v√©rifier que les ratios somment √† 1

# Mapping des variants de langues pour harmonisation
LANGUAGE_HARMONIZATION = {"be-tarask": "be"}  # harmoniser les variantes du b√©larussien

# Colonnes essentielles du corpus (dans l'ordre de priorit√©)
ESSENTIAL_COLUMNS = ["language", "title", "text"]

# Param√®tres par d√©faut des r√©pertoires
DEFAULT_MERGED_DIR = "data/processed/merged"
DEFAULT_AUGMENTED_DIR = "data/processed/augmented"
DEFAULT_OUTPUT_DIR = "data/final"

# Noms des sous-r√©pertoires de sortie
OUTPUT_SUBDIRS = {"train": "train", "validation": "validation", "test": "test"}

# Noms des fichiers de sortie
OUTPUT_FILENAMES = {
    "train": "train_corpus.csv",
    "validation": "validation_corpus.csv",
    "test": "test_corpus.csv",
}

# Param√®tres pour les statistiques
STATS_PRECISION = 1  # nb de d√©cimales pour les pourcentages
ENTROPY_PRECISION = 4  # nb de d√©cimales pour l'entropie

# Messages d'erreur standardis√©s
ERROR_MESSAGES = {
    "ratio_sum": "Les ratios doivent sommer √† 1",
    "no_data": "Aucune donn√©e disponible",
    "empty_output": "Ensemble vide",
}


# =======================
# FONCTIONS UTILITAIRES
# =======================

def validate_split_ratios(train_ratio, val_ratio, test_ratio):
    """Valide que les ratios de division sont coh√©rents

    Args:
        train_ratio (float): proportion de l'ensemble d'entra√Ænement
        val_ratio (float): proportion de l'ensemble de validation
        test_ratio (float): proportion de l'ensemble de test

    Raises:
        ValueError: si les ratios ne somment pas √† 1 ou sont invalides
    """
    # V√©rifier que tous les ratios sont des nombres positifs
    ratios = [train_ratio, val_ratio, test_ratio]
    ratio_names = ["train_ratio", "val_ratio", "test_ratio"]

    for ratio, name in zip(ratios, ratio_names):
        if not isinstance(ratio, (int, float)) or ratio < 0:
            raise ValueError(f"{name} doit √™tre un nombre positif, ici: {ratio}")
        if ratio > 1:
            raise ValueError(f"{name} ne peut pas √™tre sup√©rieur √† 1, ici: {ratio}")

    # V√©rifier que les ratios somment √† 1
    ratio_sum = sum(ratios)
    if abs(ratio_sum - 1.0) > RATIO_SUM_TOLERANCE:
        raise ValueError(f"{ERROR_MESSAGES['ratio_sum']}. Somme actuelle: {ratio_sum}")


def validate_directories(merged_dir, augmented_dir, output_dir):
    """Valide l'existence des r√©pertoires d'entr√©e et cr√©e le r√©pertoire de sortie

    Args:
        merged_dir (str): dossier des donn√©es fusionn√©es
        augmented_dir (str): dossier des donn√©es augment√©es
        output_dir (str): dossier de sortie

    Raises:
        FileNotFoundError: si un dossier d'entr√©e n'existe pas
        PermissionError: si impossible de cr√©er le dossier de sortie
    """
    # V√©rifier l'existence des dossiers d'entr√©e
    for directory, name in [
        (merged_dir, "merged_dir"),
        (augmented_dir, "augmented_dir"),
    ]:
        if not os.path.exists(directory):
            raise FileNotFoundError(f"Le r√©pertoire {name} '{directory}' n'existe pas")
        if not os.access(directory, os.R_OK):
            raise PermissionError(
                f"Pas d'autorisation de lecture sur {name} '{directory}'"
            )

    # Cr√©er le dossier de sortie et ses sous-dossiers
    try:
        for subdir_name in OUTPUT_SUBDIRS.values():
            subdir_path = os.path.join(output_dir, subdir_name)
            os.makedirs(subdir_path, exist_ok=True)
    except PermissionError:
        raise PermissionError(
            f"Impossible de cr√©er le r√©pertoire de sortie '{output_dir}'"
        )


def validate_dataframe_compatibility(dataframes_list, source_name):
    """Valide qu'une liste de DataFrames peut √™tre fusionn√©e

    Args:
        dataframes_list (list): liste de DataFrames √† valider
        source_name (str): nom de la source pour les messages d'erreur

    Raises:
        ValueError: si les DataFrames ne peuvent pas √™tre fusionn√©s

    Returns:
        bool: True si la validation r√©ussit
    """
    if not dataframes_list:
        raise ValueError(f"Aucun DataFrame trouv√© pour {source_name}")

    # V√©rifier que tous les √©l√©ments sont des DataFrames
    for i, df in enumerate(dataframes_list):
        if not isinstance(df, pd.DataFrame):
            raise ValueError(f"L'√©l√©ment {i} de {source_name} n'est pas un DataFrame")

        if df.empty:
            print(
                f"Avertissement: DataFrame vide trouv√© dans {source_name} (index {i})"
            )

    return True


def harmonize_language_codes(dataframe):
    """Harmonise les codes de langue selon les r√®gles pr√©d√©finies

    Args:
        dataframe (pd.DataFrame): DataFrame contenant une colonne 'language'

    Returns:
        pd.DataFrame: DataFrame avec codes de langue harmonis√©s
    """
    if "language" not in dataframe.columns:
        return dataframe

    # Appliquer les harmonisations
    df_copy = dataframe.copy()
    for old_code, new_code in LANGUAGE_HARMONIZATION.items():
        df_copy["language"] = df_copy["language"].replace(old_code, new_code)


def calculate_entropy(df, col="language"):
    """
    Calcule l'entropie de la distribution des valeurs dans une colonne.

    L'entropie mesure l'√©quilibre d'une distribution : une entropie plus √©lev√©e
    indique une distribution plus √©quilibr√©e entre les diff√©rentes valeurs.

    Args:
        df (pd.DataFrame): DataFrame √† analyser
        col (str): nom de la colonne √† analyser (d√©faut: 'language')

    Returns:
        float: entropie en bits (0.0 si DataFrame vide)

    Note:
        Une entropie de 0 indique qu'une seule valeur est pr√©sente.
        L'entropie maximale d√©pend du nombre de valeurs uniques.
    """
    if len(df) == 0:
        return 0.0

    counts = df[col].value_counts()
    probabilities = counts / counts.sum()
    entropy = -np.sum(probabilities * np.log2(probabilities))
    return entropy


# ====================
# FONCTION PRINCIPALE
# ====================


def split_datasets(
    merged_dir=DEFAULT_MERGED_DIR,
    augmented_dir=DEFAULT_AUGMENTED_DIR,
    output_dir=DEFAULT_OUTPUT_DIR,
    train_ratio=DEFAULT_TRAIN_RATIO,
    val_ratio=DEFAULT_VAL_RATIO,
    test_ratio=DEFAULT_TEST_RATIO,
):
    """
    Divise le corpus final en ensembles d'entra√Ænement, validation et test.

    Args:
        merged_dir (str): dossier contenant les fichiers originaux nettoy√©s
        augmented_dir (str): dossier contenant les fichiers augment√©s
        output_dir (str): dossier o√π sauvegarder les ensembles
        train_ratio (float): proportion de l'ensemble d'entra√Ænement
        val_ratio (float): proportion de l'ensemble de validation
        test_ratio (float): proportion de l'ensemble de test

    Returns:
        tuple: (train_df, val_df, test_df) - les 3 DataFrames cr√©√©s

    Raises:
        ValueError: si les param√®tres sont invalides
        FileNotFoundError: si les dossiers d'entr√©e n'existent pas
        PermissionError: si impossible d'√©crire dans le dossier de sortie
    """
    try:
        # 1. Validation des param√®tres d'entr√©e
        print("Validation des param√®tres...")
        validate_split_ratios(train_ratio, val_ratio, test_ratio)
        validate_directories(merged_dir, augmented_dir, output_dir)

        print("=== D√©but de la division du corpus ===")

        # 2. Charger les articles originaux
        print("Chargement des articles originaux...")
        original_articles = []

        try:
            for file in os.listdir(merged_dir):
                if file.endswith(".csv"):
                    file_path = os.path.join(merged_dir, file)
                    df = pd.read_csv(file_path)

                    # S'assurer que toutes les colonnes n√©cessaires existent
                    # et sont dans le bon ordre pour les articles originaux
                    if "language" not in df.columns and "page_id" in df.columns:
                        # Renommer page_id en pageid pour coh√©rence si n√©cessaire
                        if "pageid" not in df.columns and "page_id" in df.columns:
                            df = df.rename(columns={"page_id": "pageid"})

                    # Ajouter une colonne source si elle n'existe pas
                    if "source" not in df.columns:
                        df["source"] = "original"  # marquer comme original

                    original_articles.append(df)

        except Exception as e:
            raise FileNotFoundError(
                f"Erreur lors du chargement depuis {merged_dir}: {e}"
            )

        # 3. Charger les articles augment√©s
        print("Chargement des articles augment√©s...")
        augmented_articles = []

        try:
            for file in os.listdir(augmented_dir):
                if file.endswith(".csv"):
                    file_path = os.path.join(augmented_dir, file)
                    df = pd.read_csv(file_path)

                    # S'assurer que toutes les colonnes n√©cessaires existent
                    if "pageid" not in df.columns:
                        df["pageid"] = None  # ajouter une colonne pageid vide

                    augmented_articles.append(df)

        except Exception as e:
            print(
                f"Avertissement: erreur lors du chargement depuis {augmented_dir} - {e}"
            )

        # 4. Fusionner tous les articles
        all_dfs = original_articles + augmented_articles
        if all_dfs:
            # Identifier toutes les colonnes uniques dans tous les DataFrames
            all_columns = set()
            for df in all_dfs:
                all_columns.update(df.columns)

            # S'assurer que chaque DataFrame a toutes les colonnes
            standardized_dfs = []
            for df in all_dfs:
                # Ajouter les colonnes manquantes avec des valeurs None
                for col in all_columns:
                    if col not in df.columns:
                        df[col] = None
                standardized_dfs.append(df)

            # Fusionner les DataFrames standardis√©s
            full_corpus = pd.concat(standardized_dfs, ignore_index=True)

            # Harmoniser les variantes du b√©larussien
            full_corpus = harmonize_language_codes(full_corpus)

            # R√©organiser les colonnes
            # pour mettre 'language', 'title', 'text' en premier
            other_columns = [
                col for col in full_corpus.columns
                if col not in ESSENTIAL_COLUMNS
            ]
            full_corpus = full_corpus[ESSENTIAL_COLUMNS + other_columns]
        else:
            full_corpus = pd.DataFrame()

        # 5. Diviser directement par langue
        # pour obtenir une distribution √©quilibr√©e
        train_dfs = []
        val_dfs = []
        test_dfs = []

        for language in full_corpus["language"].unique():
            lang_df = full_corpus[full_corpus["language"] == language]

            # Pour chaque langue, stratifier par source si possible
            if "source" in lang_df.columns and len(lang_df["source"].unique()) > 1:
                # Grouper par source pour assurer une distribution √©quilibr√©e
                for source in lang_df["source"].unique():
                    source_df = lang_df[lang_df["source"] == source]

                    # Si on a assez d'exemples, diviser normalement
                    if len(source_df) >= MIN_EXAMPLES_FOR_NORMAL_SPLIT:
                        train_size = int(len(source_df) * train_ratio)
                        val_size = int(len(source_df) * val_ratio)

                        # M√©langer les donn√©es
                        shuffled_df = source_df.sample(
                            frac=1, random_state=42
                        ).reset_index(drop=True)

                        # Diviser
                        train_part = shuffled_df.iloc[:train_size]
                        val_part = shuffled_df.iloc[train_size:train_size+val_size]
                        test_part = shuffled_df.iloc[train_size+val_size:]

                        train_dfs.append(train_part)
                        val_dfs.append(val_part)
                        test_dfs.append(test_part)

                    elif len(source_df) >= MIN_EXAMPLES_FOR_ANY_SPLIT:
                        train_size = max(1, int(len(source_df) * train_ratio))
                        val_size = max(1, int(len(source_df) * val_ratio))

                        # M√©langer les donn√©es
                        shuffled_df = source_df.sample(
                            frac=1, random_state=42
                        ).reset_index(drop=True)

                        # Diviser
                        train_part = shuffled_df.iloc[:train_size]
                        val_part = shuffled_df.iloc[train_size:train_size+val_size]
                        test_part = shuffled_df.iloc[train_size+val_size:]

                        train_dfs.append(train_part)
                        if len(val_part) > 0:
                            val_dfs.append(val_part)
                        if len(test_part) > 0:
                            test_dfs.append(test_part)

                    else:  # tr√®s peu d'exemples, allouer √† l'entra√Ænement principalement
                        if len(source_df) == MIN_EXAMPLES_FOR_DUAL_SPLIT:
                            train_dfs.append(source_df.iloc[:1])
                            val_dfs.append(source_df.iloc[1:])
                        else:  # 1 seul exemple
                            train_dfs.append(source_df)
            else:
                # Si pas de colonne source ou une seule source,
                # diviser directement la langue
                if len(lang_df) >= MIN_EXAMPLES_FOR_NORMAL_SPLIT:
                    train_size = int(len(lang_df) * train_ratio)
                    val_size = int(len(lang_df) * val_ratio)

                    # M√©langer les donn√©es
                    shuffled_df = lang_df.sample(
                        frac=1, random_state=42
                        ).reset_index(drop=True)

                    # Diviser
                    train_part = shuffled_df.iloc[:train_size]
                    val_part = shuffled_df.iloc[train_size:train_size+val_size]
                    test_part = shuffled_df.iloc[train_size+val_size:]

                    train_dfs.append(train_part)
                    val_dfs.append(val_part)
                    test_dfs.append(test_part)

                elif len(lang_df) >= MIN_EXAMPLES_FOR_ANY_SPLIT:
                    train_size = max(1, int(len(lang_df) * train_ratio))
                    val_size = max(1, int(len(lang_df) * val_ratio))

                    # M√©langer les donn√©es
                    shuffled_df = lang_df.sample(
                        frac=1, random_state=42
                        ).reset_index(drop=True)

                    # Diviser
                    train_part = shuffled_df.iloc[:train_size]
                    val_part = shuffled_df.iloc[train_size:train_size+val_size]
                    test_part = shuffled_df.iloc[train_size+val_size:]

                    train_dfs.append(train_part)
                    if len(val_part) > 0:
                        val_dfs.append(val_part)
                    if len(test_part) > 0:
                        test_dfs.append(test_part)

                else:
                    if len(lang_df) == MIN_EXAMPLES_FOR_DUAL_SPLIT:
                        train_dfs.append(lang_df.iloc[:1])
                        val_dfs.append(lang_df.iloc[1:])
                    else:
                        train_dfs.append(lang_df)

        # 6. Combiner les ensembles
        train_df = (
            pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()
        )
        val_df = pd.concat(val_dfs, ignore_index=True) if val_dfs else pd.DataFrame()
        test_df = pd.concat(test_dfs, ignore_index=True) if test_dfs else pd.DataFrame()

        # 7. Sauvegarder les ensembles
        train_df.to_csv(
            os.path.join(
                output_dir, OUTPUT_SUBDIRS["train"], OUTPUT_FILENAMES["train"]
            ),
            index=False,
        )
        val_df.to_csv(
            os.path.join(
                output_dir,
                OUTPUT_SUBDIRS["validation"],
                OUTPUT_FILENAMES["validation"]
            ),
            index=False,
        )
        test_df.to_csv(
            os.path.join(
                output_dir,
                OUTPUT_SUBDIRS["test"],
                OUTPUT_FILENAMES["test"]
            ),
            index=False,
        )

        # 8. Afficher les statistiques
        print("\nDivision du corpus en ensembles:")
        print(f"Corpus complet: {len(full_corpus)} articles")
        print(
            f"Ensemble d'entra√Ænement: {len(train_df)} articles "
            f"({len(train_df)/len(full_corpus)*100:.{STATS_PRECISION}f}%)"
        )
        print(
            f"Ensemble de validation: {len(val_df)} articles "
            f"({len(val_df)/len(full_corpus)*100:.{STATS_PRECISION}f}%)"
        )
        print(
            f"Ensemble de test: {len(test_df)} articles "
            f"({len(test_df)/len(full_corpus)*100:.{STATS_PRECISION}f}%)"
        )

        # Statistiques par langue
        print("\nDistribution des langues dans chaque ensemble:")
        print("\nEntra√Ænement:")
        print(train_df["language"].value_counts())
        print("\nValidation:")
        print(val_df["language"].value_counts() if len(val_df) > 0 else "Ensemble vide")
        print("\nTest:")
        print(
            test_df["language"].value_counts() if len(test_df) > 0 else "Ensemble vide"
        )

        # Calculer les statistiques de longueur
        # pour v√©rifier la repr√©sentativit√©
        if "token_count" in train_df.columns and len(train_df) > 0:
            train_length_stats = train_df["token_count"].describe()

            print("\nStatistiques de longueur des textes (en tokens) par ensemble:")
            print(
                f"Entra√Ænement: moyenne={train_length_stats['mean']:.1f}, "
                f"m√©diane={train_length_stats['50%']:.1f}, "
                f"min={train_length_stats['min']:.1f}, "
                f"max={train_length_stats['max']:.1f}"
            )

            if "token_count" in val_df.columns and len(val_df) > 0:
                val_length_stats = val_df["token_count"].describe()
                print(
                    f"Validation: moyenne={val_length_stats['mean']:.1f}, "
                    f"m√©diane={val_length_stats['50%']:.1f}, "
                    f"min={val_length_stats['min']:.1f}, "
                    f"max={val_length_stats['max']:.1f}"
                )
            else:
                print("Validation: pas de statistiques disponibles")

            if "token_count" in test_df.columns and len(test_df) > 0:
                test_length_stats = test_df["token_count"].describe()
                print(
                    f"Test: moyenne={test_length_stats['mean']:.1f}, "
                    f"m√©diane={test_length_stats['50%']:.1f}, "
                    f"min={test_length_stats['min']:.1f}, "
                    f"max={test_length_stats['max']:.1f}"
                )
            else:
                print("Test: pas de statistiques disponibles")

        # V√©rifier la distribution des m√©thodes d'augmentation si applicable
        if "source" in train_df.columns:
            print("\nDistribution des m√©thodes d'augmentation:")
            print("Entra√Ænement:")
            print(train_df["source"].value_counts())

            print("\nValidation:")
            if "source" in val_df.columns and len(val_df) > 0:
                print(val_df["source"].value_counts())
            else:
                print("Pas de donn√©es disponibles")

            print("\nTest:")
            if "source" in test_df.columns and len(test_df) > 0:
                print(test_df["source"].value_counts())
            else:
                print("Pas de donn√©es disponibles")

            print("=== Division du corpus termin√©e avec succ√®s ===")
            return train_df, val_df, test_df

        # Calcul de l'entropie (utilise notre fonction utilitaire)
        train_entropy = calculate_entropy(train_df) if len(train_df) > 0 else 0.0
        val_entropy = calculate_entropy(val_df) if len(val_df) > 0 else 0.0
        test_entropy = calculate_entropy(test_df) if len(test_df) > 0 else 0.0

        print("\nEntropie de la distribution des langues (mesure d'√©quilibre):")
        print(f"Entra√Ænement: {train_entropy:.{ENTROPY_PRECISION}f} bits")
        print(f"Validation: {val_entropy:.{ENTROPY_PRECISION}f} bits")
        print(f"Test: {test_entropy:.{ENTROPY_PRECISION}f} bits")

        print("=== Division du corpus termin√©e avec succ√®s ===")
        return train_df, val_df, test_df

    except Exception as e:
        print(f"Erreur lors de la division du corpus: {e}")
        raise


# Point d'entr√©e principal du script
if __name__ == "__main__":
    """Ex√©cute la division avec les param√®tres par d√©faut"""
    try:
        print("Lancement de la division du corpus...")

        train_df, val_df, test_df = split_datasets()

        print("\n‚úÖ Division termin√©e avec succ√®s!")
        print(
            f"Train: {len(train_df)} | Validation: {len(val_df)} | Test: {len(test_df)}"
        )
        print(f"üìÅ R√©sultats sauvegard√©s dans: {DEFAULT_OUTPUT_DIR}")

    except (FileNotFoundError, PermissionError) as e:
        print(f"\n‚ùå Erreur d'acc√®s aux fichiers: {e}")
        exit(1)
    except ValueError as e:
        print(f"\n‚ùå Erreur de param√®tres: {e}")
        exit(1)
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        exit(1)
