"""Module de visualisation et d'analyse statistique pour corpus multilingues cyrilliques

Ce module fournit un ensemble d'outils pour analyser et visualiser
les caract√©ristiques d'un corpus de textes en langues cyrilliques.
Il impl√©mente plusieurs analyses linguistiques et statistiques.

Analyses principales:
    * Distribution des longueurs de textes et statistiques descriptives
    * Classification et analyse par groupes linguistiques (familles de langues)
    * Analyse de la loi de Zipf pour √©tudier les distributions de fr√©quence des mots
    * Identification des caract√®res cyrilliques distinctifs par langue
    * Visualisations comparatives multi-langues

Exemples d'utilisation :
    Analyse compl√®te d'un corpus:
        >>> from visualize_corpus import main
        >>> main()  # G√©n√®re toutes les visualisations
    Analyse cibl√©e:
        >>> from visualize_corpus import explore_corpus_stats, load_all_articles
        >>> articles = load_all_articles()
        >>> explore_corpus_stats(articles, output_dir="mes_resultats")

Organisation des donn√©es:
    Le module attend des fichiers CSV dans le format suivant:
    - fichiers nomm√©s "{code_langue}_articles.csv"
    - colonnes requises: 'language', 'text', 'token_count'
    - colonnes optionnelles: 'title', 'category', 'source'

Sortie:
    Toutes les visualisations sont sauvegard√©es dans le dossier sp√©cifi√©
    avec une r√©solution haute (300 DPI) adapt√©e.

Note m√©thodologique :
    Les analyses prennent en compte les diff√©rences
    de richesse des corpus selon les langues.
    La classification par groupes suit une approche linguistique bas√©e sur
    les familles de langues et la disponibilit√© des ressources num√©riques.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
import re
from collections import Counter
from matplotlib.ticker import FuncFormatter

# Import du module de configuration
from corpus.modules.config import get_language_group


# ============================
# CONSTANTES DE CONFIGURATION
# ============================

# Langues pour analyse approfondie
SAMPLE_LANGUAGES = [
    "ru",
    "uk",
    "be",
    "bg",
    "mk",
    "rue",
    "sr",
    "kk",
    "mn",
    "tt",
    "tg",
    "bxr",
    "ce",
    "koi",
    "mhr",
]

# Param√®tres de visualisation - tailles de figures
FIGURE_SIZES = {
    "standard": (12, 8),  # taille par d√©faut pour la plupart des graphiques
    "wide": (14, 8),  # pour les graphiques n√©cessitant plus de largeur
    "extra_wide": (16, 10),  # pour les graphiques avec beaucoup de donn√©es
    "tall": (12, 10),  # pour les graphiques n√©cessitant plus de hauteur
    "large": (20, 20),  # pour les heatmaps complexes
    "wordcloud": (16, 8),  # sp√©cifique aux nuages de mots
}

# Param√®tres de qualit√© des images
IMAGE_SETTINGS = {
    "dpi": 300,  # r√©solution pour les images sauvegard√©es
    "format": "png",  # format par d√©faut des images
    "bbox_inches": "tight",  # ajustement automatique des marges
}

# Param√®tres d'analyse statistique
ANALYSIS_PARAMS = {
    "histogram_bins": 50,  # nb de bins pour les histogrammes
    "top_languages": 10,  # nb de langues √† afficher dans les tops
    "top_categories": 10,  # nb de cat√©gories √† afficher dans les tops
    "max_words_zipf": 50,  # nb de mots pour l'analyse de Zipf
    "max_words_cloud": 100,  # nb de mots dans les nuages
    "max_chars_distinctive": 26,  # nb max de caract√®res distinctifs √† analyser
    "min_articles_for_analysis": 5,  # minimum d'articles pour inclure une langue
    "min_articles_for_zipf": 10,  # minimum d'articles pour l'analyse de Zipf
}

# Configuration des couleurs et palettes
COLOR_SCHEMES = {
    "primary": "viridis",  # palette principale
    "secondary": "Set2",  # palette secondaire pour cat√©gories
    "heatmap": "viridis",  # palette pour heatmaps
    "comparative": "deep",  # palette pour comparaisons
}

# Param√®tres de style matplotlib
MATPLOTLIB_STYLE = {
    "style": "seaborn-v0_8-whitegrid",  # style de base
    "font_family": "DejaVu Sans",  # police pour caract√®res cyrilliques
    "default_font_size": 12,  # taille de police par d√©faut
}

# R√©pertoires par d√©faut
DEFAULT_PATHS = {
    "input_pattern": "data/processed/merged/*_articles.csv",
    "output_base": "results/figures/distribution",
}

# Messages et titres standardis√©s
PLOT_TITLES = {
    "token_distribution": "Distribution des longueurs d'articles (tokens)",
    "top_languages": "Top {} des langues par nombre d'articles",
    "top_categories": "Top {} des cat√©gories th√©matiques",
    "tokens_per_language": "Distribution des tokens par article pour chaque langue",
    "zipf_analysis": "Loi de Zipf pour la langue {}",
    "zipf_comparison": "Comparaison de la loi de Zipf entre les langues",
    "distinctive_chars": "Caract√®res cyrilliques distinctifs par langue",
}


# ======================
# FONCTIONS UTILITAIRES
# ======================

def setup_matplotlib_config():
    """Configure matplotlib

    Cette fonction centralise toute la configuration matplotlib pour assurer
    une coh√©rence visuelle dans toutes les visualisations du projet.
    Elle configure la police DejaVu Sans, qui prend correctement en charge
    les caract√®res cyrilliques.

    Configurations appliqu√©es:
        - style seaborn pour les graphiques
        - police DejaVu Sans pour la prise en charge du cyrillique
        - taille de figure par d√©faut
        - taille de police lisible pour les visualisations

    Note:
        Cette fonction doit √™tre appel√©e avant toute cr√©ation de graphique
        pour garantir un rendu correct des textes en langues cyrilliques.
    """
    plt.style.use(MATPLOTLIB_STYLE["style"])
    plt.rcParams["font.family"] = MATPLOTLIB_STYLE["font_family"]
    plt.rcParams["figure.figsize"] = FIGURE_SIZES["standard"]
    plt.rcParams["font.size"] = MATPLOTLIB_STYLE["default_font_size"]


def save_plot(filename, output_dir, size_key="standard"):
    """Sauvegarde un graphique avec les param√®tres standardis√©s

    Cette fonction encapsule la logique de sauvegarde pour √©viter la r√©p√©tition
    de code et assurer une qualit√© uniforme des images produites.

    Args:
        filename (str): nom du fichier (sans extension)
        output_dir (str): dossier de destination
        size_key (str): cl√© du dictionnaire FIGURE_SIZES √† utiliser
            (par d√©faut: 'standard')

    Note sur la qualit√©:
        Les images sont sauvegard√©es en 300 DPI, ce qui assure une qualit√© optimale
        pour l'int√©gration dans des documents.

    Gestion m√©moire:
        La fonction ferme automatiquement les graphiques apr√®s sauvegarde
        pour √©viter l'accumulation en m√©moire lors de la g√©n√©ration
        de nombreuses visualisations.
    """
    filepath = f"{output_dir}/{filename}.{IMAGE_SETTINGS['format']}"
    plt.savefig(
        filepath, dpi=IMAGE_SETTINGS["dpi"], bbox_inches=IMAGE_SETTINGS["bbox_inches"]
    )
    plt.close()  # lib√©rer la m√©moire


def validate_output_directory(output_dir):
    """Valide et cr√©e le dossier de sortie si n√©cessaire

    Args:
        output_dir (str): chemin du dossier de sortie

    Raises:
        PermissionError: si impossible de cr√©er le dossier
    """
    try:
        os.makedirs(output_dir, exist_ok=True)
    except PermissionError:
        raise PermissionError(
            f"Impossible de cr√©er le dossier de sortie '{output_dir}'"
        )


def filter_languages_for_analysis(articles_df, min_articles=None):
    """Impl√©mente un seuil de qualit√© pour les analyses statistiques.

    Cette fonction √©limine les langues avec trop peu d'articles
    pour √©viter des conclusions statistiquement non significatives,
    tout en pr√©servant la diversit√© linguistique du corpus.

    Rationale m√©thodologique:
        Les analyses statistiques (loi de Zipf, distribution des caract√®res, etc.)
        n√©cessitent un volume minimal de donn√©es pour √™tre significatives. Cette
        fonction applique ce principe en filtrant intelligemment les langues.

    Args:
        articles_df (pd.DataFrame): DataFrame contenant les articles du corpus
        min_articles (int, optional): seuil minimum d'articles requis
            (si None, utilise la valeur par d√©faut de ANALYSIS_PARAMS)

    Returns:
        list: liste des codes de langues valid√©s pour l'analyse

    Note sur les seuils:
        Le seuil par d√©faut (5 articles) repr√©sente un compromis entre
        significativit√© statistique et pr√©servation de la diversit√© linguistique.
        Pour l'analyse de Zipf, un seuil plus √©lev√© (10 articles) est recommand√©.
    """
    if min_articles is None:
        min_articles = ANALYSIS_PARAMS["min_articles_for_analysis"]

    language_counts = articles_df["language"].value_counts()
    return language_counts[language_counts >= min_articles].index.tolist()


# ==================================
# FONCTION PRINCIPALE DE CHARGEMENT
# ==================================

def load_all_articles():
    """Charge tous les articles depuis les fichiers CSV avec validation robuste

    Cette fonction constitue le point d'entr√©e principal pour l'acc√®s aux donn√©es.
    Elle impl√©mente une logique robuste de chargement qui g√®re les erreurs de
    format, valide la coh√©rence des donn√©es, et assure la compatibilit√© entre
    diff√©rents formats de fichiers.

    Processus de validation:
        1. D√©tection automatique des fichiers selon le pattern de nommage
        2. Extraction des codes de langue depuis les noms de fichiers
        3. Validation de la pr√©sence des colonnes essentielles
        4. Nettoyage des articles vides ou corrompus
        5. Harmonisation des sch√©mas de donn√©es

    Format attendu des fichiers:
        - nom: "{code_langue}_articles.csv"
        - colonnes requises: 'text', plus optionnellement 'language', 'token_count'
        - encodage: UTF-8 (crucial pour les caract√®res cyrilliques)

    Returns:
        pd.DataFrame: DataFrame unifi√© contenant tous les articles valides
            avec colonnes standardis√©es - 'language', 'text', 'token_count', etc.

    Raises:
        FileNotFoundError: si aucun fichier correspondant au pattern n'est trouv√©
        ValueError: si aucun fichier CSV valide n'a pu √™tre charg√©

    Note:
        La fonction continue le traitement m√™me si certains fichiers sont
        corrompus, en affichant des avertissements informatifs. Cette approche
        permet de travailler avec des corpus partiellement incomplets.
    """
    pattern = DEFAULT_PATHS["input_pattern"]
    all_files = glob.glob(pattern)

    if not all_files:
        raise FileNotFoundError(f"Aucun fichier trouv√© avec le pattern: {pattern}")

    dataframes = []
    for file in all_files:
        try:
            # Extraire le code de langue du nom de fichier
            lang_code = os.path.basename(file).split("_")[0]

            df = pd.read_csv(file)

            # Valider les colonnes essentielles
            if "text" not in df.columns:
                print(f"Attention: Colonne 'text' manquante dans {file}")
                continue

            # Ajouter le code de langue si manquant
            if "language" not in df.columns:
                df["language"] = lang_code

            # Filtrer les articles vides
            df = df[df["text"].notna() & (df["text"] != "")]

            if len(df) > 0:
                dataframes.append(df)
                print(f"Charg√© {len(df)} articles pour {lang_code}")
            else:
                print(f"Aucun article valide dans {file}")

        except Exception as e:
            print(f"Erreur lors du chargement de {file}: {e}")
            continue

    if not dataframes:
        raise ValueError("Aucun fichier CSV valide n'a pu √™tre charg√©")

    combined_df = pd.concat(dataframes, ignore_index=True)
    print(
        f"Total: {len(combined_df)} articles, {combined_df['language'].nunique()} langues"
    )

    return combined_df


# =====================
# FONCTIONS D'ANALYSE
# =====================

def explore_corpus_stats(articles_df, output_dir=DEFAULT_PATHS["output_base"]):
    """Analyse statistique g√©n√©rale du corpus avec param√®tres standardis√©s"""
    validate_output_directory(output_dir)

    print("=== Statistiques globales du corpus ===")
    print(f"Nombre total d'articles: {len(articles_df)}")
    print(f"Nombre total de tokens: {articles_df['token_count'].sum():,}")
    print(f"Nombre de langues: {articles_df['language'].nunique()}")

    # Distribution des tokens par article
    plt.figure(figsize=FIGURE_SIZES["standard"])
    sns.histplot(
        articles_df["token_count"], bins=ANALYSIS_PARAMS["histogram_bins"], kde=True
    )
    plt.title(PLOT_TITLES["token_distribution"])
    plt.xlabel("Nombre de tokens")
    plt.ylabel("Nombre d'articles")
    plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
    save_plot("token_distribution", output_dir)

    # Top des langues
    plt.figure(figsize=FIGURE_SIZES["wide"])
    top_langs = (
        articles_df["language"].value_counts().head(ANALYSIS_PARAMS["top_languages"])
    )
    sns.barplot(
        x=top_langs.index,
        y=top_langs.values,
        hue=top_langs.index,
        palette=COLOR_SCHEMES["primary"],
        legend=False,
    )
    plt.title(PLOT_TITLES["top_languages"].format(ANALYSIS_PARAMS["top_languages"]))
    plt.xlabel("Langue")
    plt.ylabel("Nombre d'articles")
    save_plot("top10_languages_by_articles", output_dir)

    # Distribution des cat√©gories
    articles_df["main_category"] = articles_df["category"].apply(
        lambda x: str(x).split(" (")[0] if isinstance(x, str) and " (" in x else x
    )
    top_categories = (
        articles_df["main_category"]
        .value_counts()
        .head(ANALYSIS_PARAMS["top_categories"])
    )

    plt.figure(figsize=FIGURE_SIZES["wide"])
    sns.barplot(
        x=top_categories.index,
        y=top_categories.values,
        hue=top_categories.index,
        palette=COLOR_SCHEMES["secondary"],
        legend=False,
    )
    plt.title(PLOT_TITLES["top_categories"].format(ANALYSIS_PARAMS["top_categories"]))
    plt.xlabel("Cat√©gorie")
    plt.ylabel("Nombre d'articles")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    save_plot("top10_categories", output_dir)

    # Distribution des tokens par langue
    plt.figure(figsize=FIGURE_SIZES["extra_wide"])
    sns.boxplot(
        x="language", y="token_count", data=articles_df.sort_values(by="language")
    )
    plt.title(PLOT_TITLES["tokens_per_language"])
    plt.xlabel("Langue")
    plt.ylabel("Nombre de tokens")
    plt.xticks(rotation=90)
    plt.tight_layout()
    save_plot("tokens_per_language_boxplot", output_dir)

    return "Exploration des statistiques du corpus termin√©e!"


def analyze_text_characteristics(articles_df, output_dir=DEFAULT_PATHS["output_base"]):
    """Analyse des caract√©ristiques textuelles avec groupes de langues"""
    validate_output_directory(output_dir)

    # Ajouter une colonne pour le groupe de langue
    articles_df["language_group"] = articles_df["language"].apply(get_language_group)

    # Cr√©er un boxplot par groupe
    plt.figure(figsize=FIGURE_SIZES["wide"])
    sns.boxplot(
        x="language_group",
        y="token_count",
        data=articles_df,
        order=["A", "B", "C", "D"],
    )
    plt.title("Distribution des longueurs d'articles par groupe de langue")
    plt.xlabel("Groupe de langue")
    plt.ylabel("Nombre de tokens")
    save_plot("token_distribution_by_group", output_dir)

    return "Analyse des caract√©ristiques textuelles termin√©e!"


def analyze_zipf_law(articles_df, output_dir=DEFAULT_PATHS["output_base"]):
    """Analyse de la loi de Zipf pour les langues principales"""
    validate_output_directory(output_dir)

    # Filtrer les langues avec suffisamment d'articles
    valid_languages = filter_languages_for_analysis(
        articles_df, ANALYSIS_PARAMS["min_articles_for_zipf"]
    )

    # Intersection avec les langues d'√©chantillon
    languages_to_analyze = [
        lang for lang in SAMPLE_LANGUAGES if lang in valid_languages
    ]

    print(f"Analyse de la loi de Zipf pour {len(languages_to_analyze)} langues")

    for lang in languages_to_analyze:
        try:
            # Filtrer les articles pour cette langue
            lang_df = articles_df[articles_df["language"] == lang]

            # Concat√©ner tous les textes
            all_text = " ".join(lang_df["text"].fillna(""))

            # Tokeniser simplement par espaces
            words = re.findall(r"\b\w+\b", all_text.lower())

            # Compter les fr√©quences
            word_counts = Counter(words)
            most_common = word_counts.most_common(ANALYSIS_PARAMS["max_words_zipf"])

            # Cr√©er un DataFrame pour le graphique
            zipf_df = pd.DataFrame(most_common, columns=["word", "frequency"])
            zipf_df["rank"] = range(1, len(zipf_df) + 1)

            # Log-log plot d√©taill√©
            plt.figure(figsize=FIGURE_SIZES["wide"])

            # Donn√©es observ√©es
            plt.loglog(
                zipf_df["rank"],
                zipf_df["frequency"],
                "o",
                markersize=5,
                label="Donn√©es observ√©es",
                alpha=0.7,
            )

            # Loi de Zipf th√©orique
            ideal_zipf = zipf_df["frequency"].iloc[0] / zipf_df["rank"]
            plt.loglog(
                zipf_df["rank"],
                ideal_zipf,
                "r-",
                linewidth=2,
                label="Loi de Zipf id√©ale (1/rang)",
            )

            plt.title(PLOT_TITLES["zipf_analysis"].format(lang))
            plt.xlabel("Rang (log)")
            plt.ylabel("Fr√©quence (log)")
            plt.legend()
            plt.grid(True, which="both", ls="-", alpha=0.3)

            # Annotations pour les mots les plus fr√©quents
            for i in range(min(5, len(zipf_df))):
                plt.annotate(
                    zipf_df["word"][i],
                    (zipf_df["rank"][i], zipf_df["frequency"][i]),
                    xytext=(5, 5),
                    textcoords="offset points",
                )

            save_plot(f"zipf_detailed_{lang}", output_dir)
            print(f"Graphique cr√©√© pour {lang}")

        except Exception as e:
            print(f"Erreur lors de l'analyse de Zipf pour {lang}: {e}")
            continue

    return "Analyse approfondie de la loi de Zipf termin√©e!"


# ==============
# FONCTION MAIN
# ==============

def main():
    """Fonction principale pour l'analyse et la visualisation"""
    try:
        # Configuration initiale
        setup_matplotlib_config()

        output_dir = DEFAULT_PATHS["output_base"]
        validate_output_directory(output_dir)

        print("Chargement des articles...")
        articles_df = load_all_articles()

        print("\n1. Exploration des statistiques g√©n√©rales...")
        explore_corpus_stats(articles_df, output_dir=output_dir)

        print("\n2. Analyse des caract√©ristiques textuelles...")
        analyze_text_characteristics(articles_df, output_dir=output_dir)

        print("\n3. Analyse de la loi de Zipf...")
        analyze_zipf_law(articles_df, output_dir=output_dir)

        print("\n‚úÖ Toutes les visualisations ont √©t√© g√©n√©r√©es avec succ√®s!")
        print(f"üìÅ R√©sultats consultables dans le dossier '{output_dir}'")

    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse: {e}")
        raise


if __name__ == "__main__":
    main()
